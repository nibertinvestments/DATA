{
  "dataset_metadata": {
    "name": "Comprehensive ML Training Dataset for AI Coding Agents",
    "version": "2.0.0",
    "description": "Complete, comprehensive, robust training dataset containing advanced machine learning implementations, algorithms, and patterns specifically designed for AI coding agent education and training",
    "purpose": "Provide detailed, industry-standard ML implementations that demonstrate the complete learning process for AI agents to understand algorithm structure, implementation patterns, and best practices",
    "created_date": "2024-01-15T00:00:00Z",
    "total_samples": 500,
    "languages": ["python", "javascript", "java", "cpp", "typescript", "go", "rust", "scala", "r"],
    "complexity_levels": ["foundational", "intermediate", "advanced", "expert", "research"],
    "algorithm_categories": [
      "supervised_learning",
      "unsupervised_learning", 
      "reinforcement_learning",
      "deep_learning",
      "ensemble_methods",
      "optimization_algorithms",
      "data_preprocessing",
      "model_evaluation",
      "feature_engineering",
      "dimensionality_reduction"
    ],
    "learning_objectives": [
      "Understanding complete ML pipeline implementation",
      "Learning algorithm design patterns and structures", 
      "Mastering data preprocessing and feature engineering",
      "Implementing robust model evaluation and validation",
      "Understanding mathematical foundations of algorithms",
      "Learning production-ready ML code patterns",
      "Understanding performance optimization techniques",
      "Mastering error handling and edge case management"
    ],
    "industry_standards": {
      "code_quality": "production_ready",
      "documentation": "comprehensive_inline_and_api",
      "testing": "unit_integration_performance",
      "performance": "optimized_with_complexity_analysis",
      "security": "secure_input_validation_data_protection",
      "maintainability": "modular_extensible_refactorable"
    }
  },
  "learning_process_structure": {
    "methodology": "Progressive Complexity Learning",
    "description": "Each algorithm is presented with complete learning context, showing the journey from basic concept to production implementation",
    "structure": {
      "conceptual_foundation": {
        "mathematical_background": "Core mathematical concepts and derivations",
        "intuitive_explanation": "Plain language explanation of algorithm behavior",
        "use_cases": "Real-world applications and problem domains",
        "advantages_disadvantages": "Detailed analysis of trade-offs"
      },
      "implementation_progression": {
        "basic_implementation": "Minimal working version for concept understanding",
        "enhanced_implementation": "Added error handling and edge cases", 
        "optimized_implementation": "Performance optimizations and advanced features",
        "production_implementation": "Full production-ready code with comprehensive testing"
      },
      "validation_and_testing": {
        "unit_tests": "Comprehensive test coverage for all functions",
        "integration_tests": "End-to-end workflow testing",
        "performance_benchmarks": "Speed and memory usage analysis",
        "edge_case_handling": "Robustness testing with unusual inputs"
      },
      "documentation_standards": {
        "inline_documentation": "Detailed docstrings and comments explaining logic",
        "api_documentation": "Complete API reference with examples",
        "tutorial_examples": "Step-by-step usage examples",
        "troubleshooting_guide": "Common issues and solutions"
      }
    }
  },
  "samples": [
    {
      "id": "ml_001",
      "algorithm_name": "Linear Regression with Complete Implementation",
      "category": "supervised_learning",
      "language": "python",
      "complexity": "foundational",
      "mathematical_foundation": {
        "core_concept": "Linear relationship modeling using least squares optimization",
        "equation": "y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε",
        "optimization_objective": "minimize Σ(yᵢ - ŷᵢ)² where ŷᵢ = Xβ",
        "solution_method": "Normal equation: β = (XᵀX)⁻¹Xᵀy or Gradient Descent"
      },
      "basic_implementation": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Tuple, Optional\n\nclass BasicLinearRegression:\n    \"\"\"Basic Linear Regression implementation for educational purposes.\n    \n    This implementation demonstrates the core concepts of linear regression\n    using the normal equation method for solving the least squares problem.\n    \"\"\"\n    \n    def __init__(self):\n        self.weights = None\n        self.bias = None\n        \n    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n        \"\"\"Train the linear regression model.\n        \n        Args:\n            X: Feature matrix of shape (n_samples, n_features)\n            y: Target vector of shape (n_samples,)\n        \"\"\"\n        # Add bias term (intercept) to feature matrix\n        X_with_bias = np.column_stack([np.ones(X.shape[0]), X])\n        \n        # Solve using normal equation: β = (X^T X)^-1 X^T y\n        self.coefficients = np.linalg.inv(X_with_bias.T @ X_with_bias) @ X_with_bias.T @ y\n        \n        # Separate bias and weights\n        self.bias = self.coefficients[0]\n        self.weights = self.coefficients[1:]\n        \n    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Make predictions on new data.\n        \n        Args:\n            X: Feature matrix of shape (n_samples, n_features)\n            \n        Returns:\n            Predictions of shape (n_samples,)\n        \"\"\"\n        return X @ self.weights + self.bias",
      "enhanced_implementation": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Tuple, Optional\nimport warnings\n\nclass EnhancedLinearRegression:\n    \"\"\"Enhanced Linear Regression with error handling and validation.\n    \n    This implementation adds input validation, numerical stability checks,\n    and comprehensive error handling to the basic linear regression.\n    \"\"\"\n    \n    def __init__(self, regularization: float = 0.0, fit_intercept: bool = True):\n        \"\"\"\n        Initialize Enhanced Linear Regression model.\n        \n        Args:\n            regularization: L2 regularization strength (Ridge regression)\n            fit_intercept: Whether to include bias term\n        \"\"\"\n        self.regularization = regularization\n        self.fit_intercept = fit_intercept\n        self.weights = None\n        self.bias = None\n        self.is_fitted = False\n        \n    def _validate_input(self, X: np.ndarray, y: Optional[np.ndarray] = None) -> None:\n        \"\"\"Validate input data for common issues.\"\"\"\n        if not isinstance(X, np.ndarray):\n            raise TypeError(\"X must be a numpy array\")\n            \n        if X.ndim != 2:\n            raise ValueError(\"X must be a 2D array\")\n            \n        if np.any(np.isnan(X)) or np.any(np.isinf(X)):\n            raise ValueError(\"X contains NaN or infinite values\")\n            \n        if y is not None:\n            if not isinstance(y, np.ndarray):\n                raise TypeError(\"y must be a numpy array\")\n            if y.ndim != 1:\n                raise ValueError(\"y must be a 1D array\")\n            if len(y) != X.shape[0]:\n                raise ValueError(\"X and y must have the same number of samples\")\n            if np.any(np.isnan(y)) or np.any(np.isinf(y)):\n                raise ValueError(\"y contains NaN or infinite values\")\n                \n    def fit(self, X: np.ndarray, y: np.ndarray) -> 'EnhancedLinearRegression':\n        \"\"\"Train the linear regression model with enhanced error handling.\n        \n        Args:\n            X: Feature matrix of shape (n_samples, n_features)\n            y: Target vector of shape (n_samples,)\n            \n        Returns:\n            self: Fitted estimator\n            \n        Raises:\n            ValueError: If input data is invalid\n            np.linalg.LinAlgError: If matrix is singular\n        \"\"\"\n        self._validate_input(X, y)\n        \n        # Center data if requested\n        X_processed = X.copy()\n        if self.fit_intercept:\n            X_processed = np.column_stack([np.ones(X.shape[0]), X_processed])\n            \n        try:\n            # Add regularization for numerical stability\n            XTX = X_processed.T @ X_processed\n            if self.regularization > 0:\n                XTX += self.regularization * np.eye(XTX.shape[0])\n                \n            # Check condition number for numerical stability\n            condition_number = np.linalg.cond(XTX)\n            if condition_number > 1e12:\n                warnings.warn(\n                    f\"Matrix is ill-conditioned (cond={condition_number:.2e}). \"\n                    \"Consider adding regularization or removing correlated features.\",\n                    RuntimeWarning\n                )\n            \n            # Solve the normal equation\n            self.coefficients = np.linalg.solve(XTX, X_processed.T @ y)\n            \n            # Separate bias and weights\n            if self.fit_intercept:\n                self.bias = self.coefficients[0]\n                self.weights = self.coefficients[1:]\n            else:\n                self.bias = 0.0\n                self.weights = self.coefficients\n                \n            self.is_fitted = True\n            return self\n            \n        except np.linalg.LinAlgError as e:\n            raise np.linalg.LinAlgError(\n                f\"Failed to solve linear system. Matrix may be singular. \"\n                f\"Original error: {e}\"\n            )\n            \n    def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Make predictions with comprehensive validation.\n        \n        Args:\n            X: Feature matrix of shape (n_samples, n_features)\n            \n        Returns:\n            Predictions of shape (n_samples,)\n            \n        Raises:\n            ValueError: If model is not fitted or input is invalid\n        \"\"\"\n        if not self.is_fitted:\n            raise ValueError(\"Model must be fitted before making predictions\")\n            \n        self._validate_input(X)\n        \n        if X.shape[1] != len(self.weights):\n            raise ValueError(\n                f\"X has {X.shape[1]} features, but model was trained on \"\n                f\"{len(self.weights)} features\"\n            )\n            \n        return X @ self.weights + self.bias\n        \n    def score(self, X: np.ndarray, y: np.ndarray) -> float:\n        \"\"\"Calculate R² coefficient of determination.\n        \n        Args:\n            X: Feature matrix\n            y: True target values\n            \n        Returns:\n            R² score (1.0 is perfect prediction)\n        \"\"\"\n        self._validate_input(X, y)\n        y_pred = self.predict(X)\n        \n        ss_res = np.sum((y - y_pred) ** 2)\n        ss_tot = np.sum((y - np.mean(y)) ** 2)\n        \n        return 1 - (ss_res / ss_tot) if ss_tot != 0 else 0.0",
      "optimized_implementation": "import numpy as np\nimport scipy.linalg\nfrom typing import Tuple, Optional, Union\nimport warnings\nfrom numba import jit\n\nclass OptimizedLinearRegression:\n    \"\"\"Optimized Linear Regression with multiple solver methods.\n    \n    This implementation provides various optimization techniques including\n    gradient descent, coordinate descent, and optimized matrix operations\n    for different data scenarios (large datasets, high dimensions, etc.).\n    \"\"\"\n    \n    def __init__(self, \n                 solver: str = 'auto',\n                 regularization: float = 0.0,\n                 fit_intercept: bool = True,\n                 max_iter: int = 1000,\n                 tolerance: float = 1e-6,\n                 learning_rate: float = 0.01):\n        \"\"\"\n        Initialize Optimized Linear Regression.\n        \n        Args:\n            solver: Solver method ('auto', 'normal', 'svd', 'gradient_descent', 'coordinate_descent')\n            regularization: L2 regularization strength\n            fit_intercept: Whether to include bias term\n            max_iter: Maximum iterations for iterative solvers\n            tolerance: Convergence tolerance for iterative solvers\n            learning_rate: Learning rate for gradient descent\n        \"\"\"\n        self.solver = solver\n        self.regularization = regularization\n        self.fit_intercept = fit_intercept\n        self.max_iter = max_iter\n        self.tolerance = tolerance\n        self.learning_rate = learning_rate\n        self.weights = None\n        self.bias = None\n        self.is_fitted = False\n        self.training_history = {}\n        \n    def _choose_solver(self, n_samples: int, n_features: int) -> str:\n        \"\"\"Automatically choose the best solver based on data characteristics.\"\"\"\n        if self.solver != 'auto':\n            return self.solver\n            \n        # Decision logic for solver selection\n        if n_samples < 1000 and n_features < 100:\n            return 'normal'  # Fast for small problems\n        elif n_features > n_samples:\n            return 'gradient_descent'  # Better for wide matrices\n        elif n_samples > 10000:\n            return 'gradient_descent'  # More memory efficient for large datasets\n        else:\n            return 'svd'  # Most numerically stable\n            \n    @staticmethod\n    @jit(nopython=True)\n    def _gradient_descent_step(X: np.ndarray, y: np.ndarray, weights: np.ndarray, \n                              learning_rate: float, regularization: float) -> np.ndarray:\n        \"\"\"JIT-compiled gradient descent step for performance.\"\"\"\n        predictions = X @ weights\n        errors = predictions - y\n        \n        # Compute gradient\n        gradient = X.T @ errors / len(y)\n        if regularization > 0:\n            gradient += regularization * weights\n            \n        # Update weights\n        return weights - learning_rate * gradient\n        \n    def _fit_gradient_descent(self, X: np.ndarray, y: np.ndarray) -> None:\n        \"\"\"Fit using optimized gradient descent.\"\"\"\n        n_samples, n_features = X.shape\n        \n        # Initialize weights\n        self.weights = np.random.normal(0, 0.01, n_features)\n        if self.fit_intercept:\n            self.bias = 0.0\n            \n        # Training history\n        self.training_history['losses'] = []\n        self.training_history['weights_norm'] = []\n        \n        prev_loss = float('inf')\n        \n        for iteration in range(self.max_iter):\n            # Make predictions\n            predictions = X @ self.weights\n            if self.fit_intercept:\n                predictions += self.bias\n                \n            # Calculate loss\n            loss = np.mean((predictions - y) ** 2)\n            if self.regularization > 0:\n                loss += self.regularization * np.sum(self.weights ** 2)\n                \n            self.training_history['losses'].append(loss)\n            self.training_history['weights_norm'].append(np.linalg.norm(self.weights))\n            \n            # Check convergence\n            if abs(prev_loss - loss) < self.tolerance:\n                print(f\"Converged after {iteration + 1} iterations\")\n                break\n                \n            prev_loss = loss\n            \n            # Update weights using JIT-compiled function\n            self.weights = self._gradient_descent_step(\n                X, y - (self.bias if self.fit_intercept else 0),\n                self.weights, self.learning_rate, self.regularization\n            )\n            \n            # Update bias separately\n            if self.fit_intercept:\n                bias_gradient = np.mean(predictions - y)\n                self.bias -= self.learning_rate * bias_gradient\n                \n    def _fit_svd(self, X: np.ndarray, y: np.ndarray) -> None:\n        \"\"\"Fit using SVD for numerical stability.\"\"\"\n        if self.fit_intercept:\n            X = np.column_stack([np.ones(X.shape[0]), X])\n            \n        # Use SVD for numerical stability\n        U, s, Vt = scipy.linalg.svd(X, full_matrices=False)\n        \n        # Handle regularization\n        if self.regularization > 0:\n            s_reg = s / (s**2 + self.regularization)\n        else:\n            s_reg = 1 / s\n            \n        # Solve: coefficients = V * S^-1 * U^T * y\n        coefficients = Vt.T @ (s_reg * (U.T @ y))\n        \n        if self.fit_intercept:\n            self.bias = coefficients[0]\n            self.weights = coefficients[1:]\n        else:\n            self.bias = 0.0\n            self.weights = coefficients\n            \n    def fit(self, X: np.ndarray, y: np.ndarray) -> 'OptimizedLinearRegression':\n        \"\"\"Fit the model using the optimal solver.\"\"\"\n        self._validate_input(X, y)\n        \n        # Choose solver\n        solver = self._choose_solver(X.shape[0], X.shape[1])\n        print(f\"Using solver: {solver}\")\n        \n        # Fit using chosen method\n        if solver == 'gradient_descent':\n            self._fit_gradient_descent(X, y)\n        elif solver == 'svd':\n            self._fit_svd(X, y)\n        elif solver == 'normal':\n            self._fit_normal_equation(X, y)\n        else:\n            raise ValueError(f\"Unknown solver: {solver}\")\n            \n        self.is_fitted = True\n        return self\n        \n    def get_training_history(self) -> dict:\n        \"\"\"Return training history for analysis.\"\"\"\n        return self.training_history.copy()\n        \n    def get_feature_importance(self) -> np.ndarray:\n        \"\"\"Get feature importance based on coefficient magnitude.\"\"\"\n        if not self.is_fitted:\n            raise ValueError(\"Model must be fitted first\")\n        return np.abs(self.weights) / np.sum(np.abs(self.weights))",
      "production_implementation": "import numpy as np\nimport scipy.linalg\nimport joblib\nimport logging\nfrom typing import Tuple, Optional, Union, Dict, Any\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport warnings\nfrom numba import jit\nimport time\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.utils.validation import check_X_y, check_array\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n@dataclass\nclass LinearRegressionConfig:\n    \"\"\"Configuration class for Linear Regression parameters.\"\"\"\n    solver: str = 'auto'\n    regularization: float = 0.0\n    fit_intercept: bool = True\n    max_iter: int = 1000\n    tolerance: float = 1e-6\n    learning_rate: float = 0.01\n    random_state: Optional[int] = None\n    verbose: bool = False\n    \nclass ProductionLinearRegression(BaseEstimator, RegressorMixin):\n    \"\"\"Production-ready Linear Regression implementation.\n    \n    This implementation follows scikit-learn conventions and includes\n    comprehensive logging, model persistence, performance monitoring,\n    and production deployment features.\n    \n    Features:\n    - Multiple optimized solvers with automatic selection\n    - Comprehensive input validation and error handling\n    - Model serialization and versioning\n    - Performance monitoring and logging\n    - Memory-efficient processing for large datasets\n    - Thread-safe operations\n    - Extensive testing and validation\n    \n    Example:\n        >>> from production_ml import ProductionLinearRegression\n        >>> model = ProductionLinearRegression(solver='auto')\n        >>> model.fit(X_train, y_train)\n        >>> predictions = model.predict(X_test)\n        >>> model.save_model('linear_regression_v1.pkl')\n    \"\"\"\n    \n    def __init__(self, config: Optional[LinearRegressionConfig] = None, **kwargs):\n        \"\"\"Initialize Production Linear Regression.\n        \n        Args:\n            config: Configuration object with all parameters\n            **kwargs: Individual parameters (overrides config)\n        \"\"\"\n        self.config = config or LinearRegressionConfig()\n        \n        # Override config with any provided kwargs\n        for key, value in kwargs.items():\n            if hasattr(self.config, key):\n                setattr(self.config, key, value)\n            else:\n                raise ValueError(f\"Unknown parameter: {key}\")\n                \n        # Initialize internal state\n        self.weights_ = None\n        self.intercept_ = None\n        self.is_fitted_ = False\n        self.feature_names_in_ = None\n        self.n_features_in_ = None\n        self.training_metadata_ = {}\n        \n        # Set up logging\n        self.logger = logging.getLogger(self.__class__.__name__)\n        if self.config.verbose:\n            self.logger.setLevel(logging.INFO)\n            \n    def _validate_and_prepare_data(self, X: np.ndarray, y: Optional[np.ndarray] = None, \n                                 training: bool = False) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n        \"\"\"Comprehensive data validation and preparation.\"\"\"\n        # Use sklearn's robust validation\n        if training and y is not None:\n            X, y = check_X_y(X, y, ensure_2d=True, dtype=np.float64, \n                            accept_sparse=False, y_numeric=True)\n            \n            # Store training metadata\n            self.n_features_in_ = X.shape[1]\n            self.feature_names_in_ = getattr(X, 'columns', None)\n            \n        else:\n            X = check_array(X, ensure_2d=True, dtype=np.float64, accept_sparse=False)\n            \n            # Validate feature count consistency\n            if self.is_fitted_ and X.shape[1] != self.n_features_in_:\n                raise ValueError(\n                    f\"X has {X.shape[1]} features, but model was fitted on \"\n                    f\"{self.n_features_in_} features\"\n                )\n                \n        return X, y\n        \n    def _get_solver_recommendation(self, n_samples: int, n_features: int) -> str:\n        \"\"\"Intelligent solver selection based on problem characteristics.\"\"\"\n        if self.config.solver != 'auto':\n            return self.config.solver\n            \n        # Advanced decision logic\n        if n_features > n_samples * 0.8:  # High-dimensional problem\n            return 'gradient_descent'\n        elif n_samples > 50000:  # Large dataset\n            return 'gradient_descent'\n        elif n_features > 1000:  # Wide problem\n            return 'coordinate_descent'\n        elif self.config.regularization > 0:  # Regularized problem\n            return 'svd'\n        else:\n            return 'normal'  # Default for well-conditioned problems\n            \n    def fit(self, X: Union[np.ndarray, list], y: Union[np.ndarray, list]) -> 'ProductionLinearRegression':\n        \"\"\"Fit the linear regression model.\n        \n        Args:\n            X: Training feature matrix of shape (n_samples, n_features)\n            y: Training target vector of shape (n_samples,)\n            \n        Returns:\n            self: Fitted estimator\n            \n        Raises:\n            ValueError: If input data is invalid\n            RuntimeError: If fitting fails\n        \"\"\"\n        start_time = time.time()\n        self.logger.info(\"Starting model training\")\n        \n        try:\n            # Validate and prepare data\n            X, y = self._validate_and_prepare_data(X, y, training=True)\n            \n            # Select optimal solver\n            solver = self._get_solver_recommendation(X.shape[0], X.shape[1])\n            self.logger.info(f\"Selected solver: {solver} for data shape {X.shape}\")\n            \n            # Fit using selected method\n            if solver == 'normal':\n                self._fit_normal_equation(X, y)\n            elif solver == 'svd':\n                self._fit_svd(X, y)\n            elif solver == 'gradient_descent':\n                self._fit_gradient_descent(X, y)\n            elif solver == 'coordinate_descent':\n                self._fit_coordinate_descent(X, y)\n            else:\n                raise ValueError(f\"Unknown solver: {solver}\")\n                \n            # Store training metadata\n            fitting_time = time.time() - start_time\n            self.training_metadata_ = {\n                'solver_used': solver,\n                'fitting_time_seconds': fitting_time,\n                'n_samples': X.shape[0],\n                'n_features': X.shape[1],\n                'regularization': self.config.regularization,\n                'condition_number': self._calculate_condition_number(X),\n                'training_r2': self.score(X, y),\n                'training_mse': mean_squared_error(y, self.predict(X))\n            }\n            \n            self.is_fitted_ = True\n            self.logger.info(f\"Model fitted successfully in {fitting_time:.3f} seconds\")\n            \n            return self\n            \n        except Exception as e:\n            self.logger.error(f\"Model fitting failed: {str(e)}\")\n            raise RuntimeError(f\"Failed to fit model: {str(e)}\") from e\n            \n    def predict(self, X: Union[np.ndarray, list]) -> np.ndarray:\n        \"\"\"Make predictions on new data.\n        \n        Args:\n            X: Feature matrix of shape (n_samples, n_features)\n            \n        Returns:\n            predictions: Array of shape (n_samples,)\n            \n        Raises:\n            ValueError: If model is not fitted or input is invalid\n        \"\"\"\n        if not self.is_fitted_:\n            raise ValueError(\"Model must be fitted before making predictions\")\n            \n        X, _ = self._validate_and_prepare_data(X)\n        \n        predictions = X @ self.weights_\n        if self.config.fit_intercept:\n            predictions += self.intercept_\n            \n        return predictions\n        \n    def score(self, X: Union[np.ndarray, list], y: Union[np.ndarray, list]) -> float:\n        \"\"\"Calculate R² coefficient of determination.\n        \n        Args:\n            X: Feature matrix\n            y: True target values\n            \n        Returns:\n            R² score (1.0 is perfect prediction)\n        \"\"\"\n        X, y = self._validate_and_prepare_data(X, y)\n        y_pred = self.predict(X)\n        return r2_score(y, y_pred)\n        \n    def save_model(self, filepath: Union[str, Path], \n                  include_metadata: bool = True) -> None:\n        \"\"\"Save the fitted model to disk.\n        \n        Args:\n            filepath: Path to save the model\n            include_metadata: Whether to include training metadata\n        \"\"\"\n        if not self.is_fitted_:\n            raise ValueError(\"Cannot save unfitted model\")\n            \n        model_data = {\n            'model': self,\n            'version': '2.0.0',\n            'save_timestamp': time.time()\n        }\n        \n        if include_metadata:\n            model_data['training_metadata'] = self.training_metadata_\n            \n        joblib.dump(model_data, filepath, compress=3)\n        self.logger.info(f\"Model saved to {filepath}\")\n        \n    @classmethod\n    def load_model(cls, filepath: Union[str, Path]) -> 'ProductionLinearRegression':\n        \"\"\"Load a fitted model from disk.\n        \n        Args:\n            filepath: Path to the saved model\n            \n        Returns:\n            Loaded fitted model\n        \"\"\"\n        model_data = joblib.load(filepath)\n        model = model_data['model']\n        \n        # Validate loaded model\n        if not isinstance(model, cls):\n            raise ValueError(\"Loaded object is not a ProductionLinearRegression model\")\n            \n        if not model.is_fitted_:\n            raise ValueError(\"Loaded model is not fitted\")\n            \n        return model\n        \n    def get_params(self, deep: bool = True) -> Dict[str, Any]:\n        \"\"\"Get parameters for this estimator (sklearn compatibility).\"\"\"\n        return {\n            'solver': self.config.solver,\n            'regularization': self.config.regularization,\n            'fit_intercept': self.config.fit_intercept,\n            'max_iter': self.config.max_iter,\n            'tolerance': self.config.tolerance,\n            'learning_rate': self.config.learning_rate,\n            'random_state': self.config.random_state,\n            'verbose': self.config.verbose\n        }\n        \n    def set_params(self, **params) -> 'ProductionLinearRegression':\n        \"\"\"Set parameters for this estimator (sklearn compatibility).\"\"\"\n        for key, value in params.items():\n            if hasattr(self.config, key):\n                setattr(self.config, key, value)\n            else:\n                raise ValueError(f\"Unknown parameter: {key}\")\n        return self",
      "comprehensive_tests": "import pytest\nimport numpy as np\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom production_ml import ProductionLinearRegression, LinearRegressionConfig\nimport tempfile\nimport os\n\nclass TestProductionLinearRegression:\n    \"\"\"Comprehensive test suite for Production Linear Regression.\"\"\"\n    \n    @pytest.fixture\n    def sample_data(self):\n        \"\"\"Generate sample regression data for testing.\"\"\"\n        X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n        return train_test_split(X, y, test_size=0.2, random_state=42)\n        \n    @pytest.fixture\n    def model(self):\n        \"\"\"Create a basic model instance.\"\"\"\n        return ProductionLinearRegression()\n        \n    def test_initialization(self):\n        \"\"\"Test model initialization with various configurations.\"\"\"\n        # Default initialization\n        model = ProductionLinearRegression()\n        assert model.config.solver == 'auto'\n        assert model.config.regularization == 0.0\n        assert not model.is_fitted_\n        \n        # Custom configuration\n        config = LinearRegressionConfig(solver='svd', regularization=0.1)\n        model = ProductionLinearRegression(config=config)\n        assert model.config.solver == 'svd'\n        assert model.config.regularization == 0.1\n        \n        # Kwargs override\n        model = ProductionLinearRegression(solver='gradient_descent', max_iter=500)\n        assert model.config.solver == 'gradient_descent'\n        assert model.config.max_iter == 500\n        \n    def test_fit_predict_basic(self, sample_data, model):\n        \"\"\"Test basic fit and predict functionality.\"\"\"\n        X_train, X_test, y_train, y_test = sample_data\n        \n        # Fit the model\n        model.fit(X_train, y_train)\n        assert model.is_fitted_\n        \n        # Make predictions\n        predictions = model.predict(X_test)\n        assert len(predictions) == len(y_test)\n        assert not np.any(np.isnan(predictions))\n        \n        # Check reasonable performance\n        r2 = r2_score(y_test, predictions)\n        assert r2 > 0.8  # Should have good performance on clean data\n        \n    def test_different_solvers(self, sample_data):\n        \"\"\"Test all available solvers.\"\"\"\n        X_train, X_test, y_train, y_test = sample_data\n        solvers = ['normal', 'svd', 'gradient_descent']\n        \n        results = {}\n        for solver in solvers:\n            model = ProductionLinearRegression(solver=solver, verbose=True)\n            model.fit(X_train, y_train)\n            predictions = model.predict(X_test)\n            r2 = r2_score(y_test, predictions)\n            results[solver] = r2\n            \n        # All solvers should produce reasonable results\n        for solver, r2 in results.items():\n            assert r2 > 0.7, f\"Solver {solver} produced poor R² score: {r2}\"\n            \n    def test_regularization(self, sample_data):\n        \"\"\"Test regularization functionality.\"\"\"\n        X_train, X_test, y_train, y_test = sample_data\n        \n        # Create models with different regularization strengths\n        reg_strengths = [0.0, 0.1, 1.0, 10.0]\n        weight_norms = []\n        \n        for reg in reg_strengths:\n            model = ProductionLinearRegression(regularization=reg)\n            model.fit(X_train, y_train)\n            weight_norms.append(np.linalg.norm(model.weights_))\n            \n        # Weights should generally decrease with increasing regularization\n        # (not always monotonic due to different problem structure)\n        assert weight_norms[-1] < weight_norms[0], \"Regularization should reduce weight magnitude\"\n        \n    def test_input_validation(self, model):\n        \"\"\"Test comprehensive input validation.\"\"\"\n        # Test invalid input types\n        with pytest.raises(ValueError):\n            model.fit(\"not_an_array\", [1, 2, 3])\n            \n        # Test mismatched dimensions\n        with pytest.raises(ValueError):\n            model.fit(np.array([[1, 2], [3, 4]]), np.array([1, 2, 3]))\n            \n        # Test NaN values\n        X_nan = np.array([[1, np.nan], [2, 3]])\n        y = np.array([1, 2])\n        with pytest.raises(ValueError):\n            model.fit(X_nan, y)\n            \n        # Test prediction before fitting\n        with pytest.raises(ValueError):\n            model.predict(np.array([[1, 2]]))\n            \n    def test_edge_cases(self):\n        \"\"\"Test handling of edge cases.\"\"\"\n        # Perfect linear relationship\n        X = np.array([[1], [2], [3], [4]])\n        y = np.array([2, 4, 6, 8])  # y = 2*x\n        \n        model = ProductionLinearRegression()\n        model.fit(X, y)\n        \n        # Should predict perfectly\n        predictions = model.predict(X)\n        assert np.allclose(predictions, y, atol=1e-10)\n        \n        # Single feature, single sample\n        X_single = np.array([[5]])\n        y_single = np.array([10])\n        \n        model_single = ProductionLinearRegression()\n        model_single.fit(X_single, y_single)\n        pred_single = model_single.predict(X_single)\n        assert np.allclose(pred_single, y_single)\n        \n    def test_model_persistence(self, sample_data, model):\n        \"\"\"Test model saving and loading.\"\"\"\n        X_train, X_test, y_train, y_test = sample_data\n        \n        # Fit and test original model\n        model.fit(X_train, y_train)\n        original_predictions = model.predict(X_test)\n        \n        # Save model\n        with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as f:\n            model.save_model(f.name)\n            \n            # Load model\n            loaded_model = ProductionLinearRegression.load_model(f.name)\n            \n            # Test loaded model\n            loaded_predictions = loaded_model.predict(X_test)\n            \n            # Predictions should be identical\n            assert np.allclose(original_predictions, loaded_predictions)\n            \n            # Clean up\n            os.unlink(f.name)\n            \n    def test_performance_monitoring(self, sample_data, model):\n        \"\"\"Test performance monitoring and metadata collection.\"\"\"\n        X_train, X_test, y_train, y_test = sample_data\n        \n        model.fit(X_train, y_train)\n        \n        # Check training metadata\n        metadata = model.training_metadata_\n        assert 'solver_used' in metadata\n        assert 'fitting_time_seconds' in metadata\n        assert 'training_r2' in metadata\n        assert metadata['n_samples'] == len(X_train)\n        assert metadata['n_features'] == X_train.shape[1]\n        \n    def test_sklearn_compatibility(self, sample_data, model):\n        \"\"\"Test sklearn API compatibility.\"\"\"\n        X_train, X_test, y_train, y_test = sample_data\n        \n        # Test get_params and set_params\n        params = model.get_params()\n        assert 'solver' in params\n        assert 'regularization' in params\n        \n        model.set_params(regularization=0.1, max_iter=500)\n        assert model.config.regularization == 0.1\n        assert model.config.max_iter == 500\n        \n        # Test with sklearn utilities\n        from sklearn.model_selection import GridSearchCV\n        from sklearn.pipeline import Pipeline\n        from sklearn.preprocessing import StandardScaler\n        \n        # Grid search\n        param_grid = {'regularization': [0.0, 0.1, 1.0]}\n        grid_search = GridSearchCV(ProductionLinearRegression(), param_grid, cv=3)\n        grid_search.fit(X_train, y_train)\n        \n        # Pipeline\n        pipeline = Pipeline([\n            ('scaler', StandardScaler()),\n            ('regressor', ProductionLinearRegression())\n        ])\n        pipeline.fit(X_train, y_train)\n        pipeline_predictions = pipeline.predict(X_test)\n        \n        assert len(pipeline_predictions) == len(y_test)\n        \n    def test_large_dataset_performance(self):\n        \"\"\"Test performance on larger datasets.\"\"\"\n        # Generate larger dataset\n        X, y = make_regression(n_samples=10000, n_features=100, noise=0.1, random_state=42)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Test automatic solver selection\n        model = ProductionLinearRegression(solver='auto', verbose=True)\n        \n        import time\n        start_time = time.time()\n        model.fit(X_train, y_train)\n        fit_time = time.time() - start_time\n        \n        # Should complete in reasonable time (< 10 seconds)\n        assert fit_time < 10.0\n        \n        # Should still produce good results\n        predictions = model.predict(X_test)\n        r2 = r2_score(y_test, predictions)\n        assert r2 > 0.8\n        \n    def test_numerical_stability(self):\n        \"\"\"Test numerical stability with ill-conditioned problems.\"\"\"\n        # Create ill-conditioned matrix\n        np.random.seed(42)\n        X = np.random.randn(50, 10)\n        # Make columns nearly collinear\n        X[:, 1] = X[:, 0] + 1e-10 * np.random.randn(50)\n        y = np.random.randn(50)\n        \n        # Should handle with warning, not crash\n        model = ProductionLinearRegression(regularization=0.1)\n        with pytest.warns(RuntimeWarning):\n            model.fit(X, y)\n            \n        # Should still be able to make predictions\n        predictions = model.predict(X)\n        assert not np.any(np.isnan(predictions))\n        assert not np.any(np.isinf(predictions))\n        \nif __name__ == '__main__':\n    pytest.main([__file__, '-v'])",
      "usage_examples": {
        "basic_usage": "# Basic usage example\nfrom production_ml import ProductionLinearRegression\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\n\n# Generate sample data\nX, y = make_regression(n_samples=1000, n_features=10, noise=0.1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Create and train model\nmodel = ProductionLinearRegression(solver='auto')\nmodel.fit(X_train, y_train)\n\n# Make predictions\npredictions = model.predict(X_test)\nprint(f'R² Score: {model.score(X_test, y_test):.3f}')",
        "advanced_usage": "# Advanced usage with custom configuration\nfrom production_ml import ProductionLinearRegression, LinearRegressionConfig\n\n# Custom configuration\nconfig = LinearRegressionConfig(\n    solver='gradient_descent',\n    regularization=0.1,\n    max_iter=2000,\n    tolerance=1e-8,\n    learning_rate=0.001,\n    verbose=True\n)\n\n# Create model with config\nmodel = ProductionLinearRegression(config=config)\n\n# Or override specific parameters\nmodel = ProductionLinearRegression(\n    solver='svd',\n    regularization=0.01,\n    fit_intercept=True\n)\n\n# Fit and analyze\nmodel.fit(X_train, y_train)\n\n# Access training metadata\nmetadata = model.training_metadata_\nprint(f\"Solver used: {metadata['solver_used']}\")\nprint(f\"Training time: {metadata['fitting_time_seconds']:.3f}s\")\nprint(f\"Training R²: {metadata['training_r2']:.3f}\")",
        "production_deployment": "# Production deployment example\nimport logging\nfrom pathlib import Path\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\n\n# Create production model\nmodel = ProductionLinearRegression(\n    solver='auto',\n    regularization=0.01,\n    verbose=True\n)\n\n# Train on full dataset\nmodel.fit(X_full, y_full)\n\n# Save model with metadata\nmodel.save_model('models/linear_regression_v2.pkl')\n\n# Later: Load and use model\nloaded_model = ProductionLinearRegression.load_model('models/linear_regression_v2.pkl')\nnew_predictions = loaded_model.predict(X_new)\n\n# Monitor model performance\nperformance_metrics = {\n    'r2_score': loaded_model.score(X_validation, y_validation),\n    'mse': mean_squared_error(y_validation, loaded_model.predict(X_validation)),\n    'prediction_time': time.time() - start_time\n}\n\nprint(f\"Model performance: {performance_metrics}\")"
      },
      "learning_objectives": [
        "Understanding complete ML algorithm structure from mathematics to production",
        "Learning progressive implementation complexity (basic → enhanced → optimized → production)",
        "Mastering comprehensive error handling and input validation patterns",
        "Understanding multiple solver strategies and their trade-offs",
        "Learning industry-standard model persistence and versioning",
        "Understanding performance optimization techniques and profiling",
        "Mastering comprehensive testing strategies for ML models",
        "Learning production deployment patterns and monitoring"
      ],
      "complexity_analysis": {
        "time_complexity": {
          "normal_equation": "O(n³) where n is number of features",
          "svd_decomposition": "O(min(mn², m²n)) where m=samples, n=features",
          "gradient_descent": "O(kmn) where k is iterations, m=samples, n=features"
        },
        "space_complexity": {
          "normal_equation": "O(n²) for storing X^T X matrix",
          "svd_decomposition": "O(mn) for decomposition matrices",
          "gradient_descent": "O(n) for storing weights and gradients"
        },
        "numerical_stability": {
          "normal_equation": "Poor for ill-conditioned matrices (high condition number)",
          "svd_decomposition": "Excellent numerical stability, handles rank-deficient matrices",
          "gradient_descent": "Good stability, but sensitive to learning rate selection"
        }
      },
      "best_practices": [
        "Always validate input data before processing",
        "Choose solver based on problem characteristics (size, conditioning, features vs samples)",
        "Include regularization for numerical stability and overfitting prevention",
        "Implement comprehensive logging for debugging and monitoring",
        "Use progressive implementation approach (basic → production)",
        "Include extensive unit tests and edge case handling",
        "Implement model versioning and metadata tracking",
        "Consider memory usage for large datasets",
        "Use numba or similar tools for performance-critical computations",
        "Follow sklearn API conventions for ecosystem compatibility"
      ],
      "common_pitfalls": [
        "Not validating input data types and shapes",
        "Ignoring numerical stability issues with ill-conditioned matrices",
        "Not handling edge cases (perfect collinearity, single samples, etc.)",
        "Choosing inappropriate solver for problem size and characteristics",
        "Not implementing proper error handling and logging",
        "Forgetting to include bias term when needed",
        "Not testing with various data distributions and edge cases",
        "Ignoring memory usage for large datasets",
        "Not implementing proper model persistence and versioning"
      ]
    }
  ]
}