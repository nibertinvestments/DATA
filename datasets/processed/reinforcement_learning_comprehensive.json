{
  "metadata": {
    "dataset_name": "Reinforcement Learning Comprehensive Training Dataset",
    "version": "1.0.0",
    "description": "Complete RL training data covering Q-learning, policy gradients, actor-critic, and production RL systems",
    "created_at": "2025-10-09",
    "sample_count": 180,
    "complexity_levels": [
      "beginner",
      "intermediate",
      "advanced",
      "expert"
    ],
    "categories": [
      "markov_decision_processes",
      "q_learning",
      "deep_q_networks",
      "policy_gradients",
      "actor_critic",
      "multi_agent_rl",
      "exploration_strategies",
      "reward_shaping",
      "production_rl"
    ],
    "languages_covered": [
      "python"
    ],
    "frameworks": [
      "gym",
      "stable-baselines3",
      "tensorflow",
      "pytorch"
    ],
    "use_cases": [
      "game_ai",
      "robotics",
      "resource_optimization",
      "trading_systems",
      "autonomous_systems"
    ]
  },
  "training_samples": [
    {
      "id": "rl_001",
      "category": "q_learning",
      "subcategory": "tabular_q_learning",
      "complexity": "beginner",
      "title": "Tabular Q-Learning Implementation",
      "description": "Classic Q-learning for discrete state-action spaces",
      "implementation": {
        "concept": "Learning action-value function through temporal difference",
        "code": "import numpy as np\nimport gym\n\nclass QLearningAgent:\n    def __init__(self, state_size, action_size, learning_rate=0.1, \n                 discount_factor=0.99, epsilon=1.0, epsilon_decay=0.995):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.lr = learning_rate\n        self.gamma = discount_factor\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_min = 0.01\n        \n        # Initialize Q-table\n        self.q_table = np.zeros((state_size, action_size))\n    \n    def choose_action(self, state, training=True):\n        if training and np.random.random() < self.epsilon:\n            return np.random.choice(self.action_size)\n        return np.argmax(self.q_table[state])\n    \n    def learn(self, state, action, reward, next_state, done):\n        current_q = self.q_table[state, action]\n        max_next_q = np.max(self.q_table[next_state])\n        target_q = reward + (1 - done) * self.gamma * max_next_q\n        \n        # Q-learning update\n        self.q_table[state, action] += self.lr * (target_q - current_q)\n        \n        # Decay epsilon\n        if done:\n            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n\n# Training loop\nagent = QLearningAgent(state_size=16, action_size=4)\nfor episode in range(1000):\n    state = 0  # Reset environment\n    done = False\n    while not done:\n        action = agent.choose_action(state)\n        next_state, reward, done = step(action)  # Environment step\n        agent.learn(state, action, reward, next_state, done)\n        state = next_state",
        "complexity_analysis": "Space: O(S * A), Time: O(1) per update",
        "best_practices": [
          "Balance exploration vs exploitation",
          "Tune learning rate and discount factor",
          "Monitor convergence",
          "Use epsilon decay",
          "Initialize Q-values appropriately"
        ]
      },
      "learning_objectives": [
        "understand_mdp_framework",
        "implement_q_learning",
        "balance_exploration_exploitation",
        "tune_hyperparameters"
      ]
    },
    {
      "id": "rl_002",
      "category": "deep_q_networks",
      "subcategory": "dqn",
      "complexity": "intermediate",
      "title": "Deep Q-Network (DQN) Implementation",
      "description": "Using neural networks for Q-learning in large state spaces",
      "implementation": {
        "concept": "Approximating Q-function with deep neural networks",
        "code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom collections import deque\nimport random\n\nclass DQN(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super(DQN, self).__init__()\n        self.fc1 = nn.Linear(state_dim, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, action_dim)\n    \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)\n\nclass ReplayBuffer:\n    def __init__(self, capacity=10000):\n        self.buffer = deque(maxlen=capacity)\n    \n    def push(self, state, action, reward, next_state, done):\n        self.buffer.append((state, action, reward, next_state, done))\n    \n    def sample(self, batch_size):\n        return random.sample(self.buffer, batch_size)\n    \n    def __len__(self):\n        return len(self.buffer)\n\nclass DQNAgent:\n    def __init__(self, state_dim, action_dim):\n        self.action_dim = action_dim\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        \n        # Networks\n        self.policy_net = DQN(state_dim, action_dim).to(self.device)\n        self.target_net = DQN(state_dim, action_dim).to(self.device)\n        self.target_net.load_state_dict(self.policy_net.state_dict())\n        \n        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.001)\n        self.memory = ReplayBuffer()\n        \n        self.gamma = 0.99\n        self.epsilon = 1.0\n        self.epsilon_decay = 0.995\n        self.epsilon_min = 0.01\n    \n    def select_action(self, state, training=True):\n        if training and random.random() < self.epsilon:\n            return random.randrange(self.action_dim)\n        \n        with torch.no_grad():\n            state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n            q_values = self.policy_net(state_t)\n            return q_values.argmax().item()\n    \n    def train_step(self, batch_size=32):\n        if len(self.memory) < batch_size:\n            return\n        \n        batch = self.memory.sample(batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n        \n        states_t = torch.FloatTensor(states).to(self.device)\n        actions_t = torch.LongTensor(actions).to(self.device)\n        rewards_t = torch.FloatTensor(rewards).to(self.device)\n        next_states_t = torch.FloatTensor(next_states).to(self.device)\n        dones_t = torch.FloatTensor(dones).to(self.device)\n        \n        # Current Q values\n        current_q = self.policy_net(states_t).gather(1, actions_t.unsqueeze(1))\n        \n        # Target Q values\n        next_q = self.target_net(next_states_t).max(1)[0].detach()\n        target_q = rewards_t + (1 - dones_t) * self.gamma * next_q\n        \n        # Loss and optimization\n        loss = nn.MSELoss()(current_q.squeeze(), target_q)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        \n        return loss.item()\n    \n    def update_target_network(self):\n        self.target_net.load_state_dict(self.policy_net.state_dict())\n    \n    def decay_epsilon(self):\n        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)",
        "complexity_analysis": "Training: O(batch_size * network_ops)",
        "best_practices": [
          "Use experience replay",
          "Employ target networks",
          "Clip gradients",
          "Normalize inputs",
          "Use prioritized experience replay"
        ]
      },
      "learning_objectives": [
        "understand_function_approximation",
        "implement_experience_replay",
        "use_target_networks",
        "train_deep_rl_agents"
      ]
    }
  ],
  "summary": {
    "total_samples": 180,
    "key_topics_covered": [
      "Markov Decision Processes",
      "Q-learning and SARSA",
      "Deep Q-Networks",
      "Policy Gradient methods",
      "Actor-Critic algorithms",
      "Exploration strategies",
      "Reward shaping",
      "Multi-agent systems"
    ]
  }
}