{
  "metadata": {
    "name": "database_patterns",
    "description": "Database design patterns and optimization techniques",
    "total_patterns": 2,
    "created_at": "2025-10-07T23:15:11.513989"
  },
  "patterns": [
    {
      "id": "db_001",
      "pattern_name": "Connection Pooling",
      "description": "Reuse database connections efficiently",
      "problem": "Creating new connection for each query is expensive",
      "solution": {
        "python": "from sqlalchemy import create_engine\nfrom sqlalchemy.pool import QueuePool\n\n# Create engine with connection pooling\nengine = create_engine(\n    'postgresql://user:pass@localhost/db',\n    poolclass=QueuePool,\n    pool_size=10,  # Keep 10 connections\n    max_overflow=20,  # Allow 20 additional connections if needed\n    pool_pre_ping=True,  # Verify connection before use\n    pool_recycle=3600  # Recycle connections after 1 hour\n)\n\n# Use connection from pool\nwith engine.connect() as conn:\n    result = conn.execute(\"SELECT * FROM users\")\n    # Connection automatically returned to pool"
      },
      "benefits": [
        "Reduced connection overhead",
        "Better resource utilization",
        "Improved application performance",
        "Automatic connection management"
      ]
    },
    {
      "id": "db_002",
      "pattern_name": "Batch Operations",
      "description": "Process multiple records efficiently",
      "problem": "Individual inserts are slow for bulk data",
      "comparison": {
        "slow": "# Slow: Individual inserts\nfor user in users:\n    cursor.execute(\n        \"INSERT INTO users (name, email) VALUES (?, ?)\",\n        (user['name'], user['email'])\n    )\n    conn.commit()  # Commit each insert",
        "fast": "# Fast: Batch insert\ncursor.executemany(\n    \"INSERT INTO users (name, email) VALUES (?, ?)\",\n    [(u['name'], u['email']) for u in users]\n)\nconn.commit()  # Single commit for all"
      },
      "performance_gain": "10-100x faster for large datasets"
    }
  ]
}