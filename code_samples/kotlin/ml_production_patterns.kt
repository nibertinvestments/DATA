/*
Production-Ready Machine Learning Patterns in Kotlin
===================================================

This module demonstrates industry-standard ML patterns in Kotlin with proper
Android integration, coroutines, and production deployment considerations for AI training datasets.

Key Features:
- Kotlin coroutines for asynchronous operations
- Type-safe builders and DSL patterns
- Android integration patterns with Room and LiveData
- Multiplatform compatibility (JVM, Android, Native)
- Comprehensive error handling with sealed classes
- Data classes with validation
- Extensive documentation for AI learning
- Production-ready patterns with performance monitoring

Author: AI Training Dataset
License: MIT
*/

package ml.production.patterns

import kotlinx.coroutines.*
import kotlinx.coroutines.sync.Mutex
import kotlinx.coroutines.sync.withLock
import kotlin.collections.mutableListOf
import kotlin.math.*
import kotlin.random.Random
import kotlinx.serialization.*
import kotlinx.serialization.json.*
import java.io.File
import java.util.concurrent.ConcurrentHashMap
import java.util.concurrent.atomic.AtomicBoolean
import java.util.concurrent.atomic.AtomicInteger

// MARK: - Type Definitions

typealias Matrix = List<List<Double>>
typealias Vector = List<Double>
typealias MutableMatrix = MutableList<MutableList<Double>>
typealias MutableVector = MutableList<Double>

// MARK: - Error Hierarchy

sealed class MLException(message: String, cause: Throwable? = null) : Exception(message, cause) {
    abstract val context: String
}

data class DataValidationException(
    override val message: String,
    val validationErrors: List<String> = emptyList(),
    override val context: String = "data_validation"
) : MLException(message)

data class ModelTrainingException(
    override val message: String,
    val iterationsFailed: Int? = null,
    override val context: String = "model_training"
) : MLException(message)

data class ModelPredictionException(
    override val message: String,
    override val context: String = "model_prediction"
) : MLException(message)

data class FeatureEngineeringException(
    override val message: String,
    override val context: String = "feature_engineering"
) : MLException(message)

// MARK: - Data Classes

@Serializable
data class ValidationResult(
    val isValid: Boolean,
    val errors: List<String> = emptyList(),
    val warnings: List<String> = emptyList(),
    val totalSamples: Int,
    val totalFeatures: Int,
    val missingValues: Int,
    val missingValueRatio: Double,
    val featureMissingCounts: Map<String, Int> = emptyMap(),
    val featureStatistics: Map<String, FeatureStatistics> = emptyMap()
)

@Serializable
data class FeatureStatistics(
    val min: Double,
    val max: Double,
    val mean: Double,
    val standardDeviation: Double,
    val variance: Double,
    val skewness: Double,
    val kurtosis: Double
)

@Serializable
data class ModelMetrics(
    val mse: Double,
    val rmse: Double,
    val mae: Double,
    val rSquared: Double,
    val trainingTime: Long, // milliseconds
    val predictionTime: Long, // milliseconds
    val iterationsCompleted: Int,
    val convergenceValue: Double,
    val trainingHistory: List<Double>
)

@Serializable
data class TrainingConfig(
    val learningRate: Double = 0.01,
    val maxIterations: Int = 1000,
    val convergenceThreshold: Double = 1e-6,
    val validationSplit: Double = 0.2,
    val enableEarlyStopping: Boolean = true,
    val earlyStoppingPatience: Int = 10,
    val enableRegularization: Boolean = false,
    val regularizationStrength: Double = 0.01,
    val batchSize: Int = 32
) {
    init {
        require(learningRate > 0) { "Learning rate must be positive" }
        require(maxIterations > 0) { "Max iterations must be positive" }
        require(convergenceThreshold > 0) { "Convergence threshold must be positive" }
        require(validationSplit in 0.0..1.0) { "Validation split must be between 0 and 1" }
        require(earlyStoppingPatience > 0) { "Early stopping patience must be positive" }
        require(regularizationStrength >= 0) { "Regularization strength must be non-negative" }
        require(batchSize > 0) { "Batch size must be positive" }
    }
}

data class FeatureTransformResult(
    val transformedFeatures: Matrix,
    val featureMeans: Vector? = null,
    val featureStds: Vector? = null,
    val transformationParameters: Map<String, Any> = emptyMap()
)

// MARK: - Logging Interface and Implementation

interface MLLogger {
    suspend fun log(level: LogLevel, message: String)
    suspend fun logException(error: Throwable, context: String)
}

enum class LogLevel(val displayName: String) {
    DEBUG("DEBUG"),
    INFO("INFO"),
    WARNING("WARNING"),
    ERROR("ERROR"),
    CRITICAL("CRITICAL")
}

class AsyncConsoleLogger : MLLogger {
    private val logChannel = kotlinx.coroutines.channels.Channel<LogEntry>(capacity = 100)
    private val coroutineScope = CoroutineScope(Dispatchers.IO + SupervisorJob())
    
    data class LogEntry(
        val level: LogLevel,
        val message: String,
        val timestamp: Long = System.currentTimeMillis()
    )
    
    init {
        // Start log processing coroutine
        coroutineScope.launch {
            for (entry in logChannel) {
                val timestamp = java.time.Instant.ofEpochMilli(entry.timestamp)
                println("[${timestamp}] [${entry.level.displayName}] ${entry.message}")
            }
        }
    }
    
    override suspend fun log(level: LogLevel, message: String) {
        logChannel.trySend(LogEntry(level, message))
    }
    
    override suspend fun logException(error: Throwable, context: String) {
        log(LogLevel.ERROR, "$context: ${error.message}")
        if (error is MLException) {
            log(LogLevel.DEBUG, "ML Error Context: ${error.context}")
        }
        error.stackTrace?.let { stackTrace ->
            log(LogLevel.DEBUG, "Stack Trace: ${stackTrace.joinToString("\n")}")
        }
    }
    
    fun close() {
        logChannel.close()
        coroutineScope.cancel()
    }
}

// MARK: - Performance Monitoring

class PerformanceMonitor(
    private val operationName: String,
    private val logger: MLLogger
) {
    private val startTime = System.currentTimeMillis()
    
    suspend fun dispose() {
        val endTime = System.currentTimeMillis()
        val duration = endTime - startTime
        logger.log(LogLevel.INFO, "[PERFORMANCE] $operationName completed in ${duration}ms")
    }
    
    val elapsedTime: Long
        get() = System.currentTimeMillis() - startTime
}

// MARK: - Mathematical Utilities

object MathUtils {
    
    /**
     * Optimized matrix multiplication using blocked algorithm
     */
    suspend fun matrixMultiply(a: Matrix, b: Matrix): Matrix = withContext(Dispatchers.Default) {
        require(a.isNotEmpty() && b.isNotEmpty()) { "Matrices cannot be empty" }
        require(a.first().size == b.size) { "Matrix dimensions don't match for multiplication" }
        
        val rowsA = a.size
        val colsA = a.first().size
        val colsB = b.first().size
        
        val result = MutableList(rowsA) { MutableList(colsB) { 0.0 } }
        
        // Parallel computation using coroutines
        coroutineScope {
            for (i in 0 until rowsA) {
                launch {
                    for (j in 0 until colsB) {
                        var sum = 0.0
                        for (k in 0 until colsA) {
                            sum += a[i][k] * b[k][j]
                        }
                        result[i][j] = sum
                    }
                }
            }
        }
        
        result
    }
    
    /**
     * Vectorized dot product calculation
     */
    fun dotProduct(a: Vector, b: Vector): Double {
        require(a.size == b.size) { "Vector lengths must match" }
        return a.zip(b) { x, y -> x * y }.sum()
    }
    
    /**
     * Calculate comprehensive statistics for a vector
     */
    suspend fun calculateStatistics(values: Vector): FeatureStatistics = withContext(Dispatchers.Default) {
        val validValues = values.filter { it.isFinite() && !it.isNaN() }
        
        if (validValues.isEmpty()) {
            return@withContext FeatureStatistics(
                min = Double.NaN, max = Double.NaN, mean = Double.NaN,
                standardDeviation = Double.NaN, variance = Double.NaN,
                skewness = Double.NaN, kurtosis = Double.NaN
            )
        }
        
        val count = validValues.size.toDouble()
        val min = validValues.minOrNull() ?: Double.NaN
        val max = validValues.maxOrNull() ?: Double.NaN
        val mean = validValues.average()
        
        val variance = validValues.map { (it - mean).pow(2) }.average()
        val standardDeviation = sqrt(variance)
        
        // Calculate skewness and kurtosis
        val normalizedValues = validValues.map { (it - mean) / standardDeviation }
        val skewness = normalizedValues.map { it.pow(3) }.average()
        val kurtosis = normalizedValues.map { it.pow(4) }.average() - 3.0 // Excess kurtosis
        
        FeatureStatistics(
            min = min, max = max, mean = mean,
            standardDeviation = standardDeviation, variance = variance,
            skewness = skewness, kurtosis = kurtosis
        )
    }
    
    /**
     * Generate synthetic regression dataset with configurable parameters
     */
    suspend fun generateRegressionDataset(
        samples: Int,
        features: Int,
        noiseLevel: Double = 0.1,
        seed: Int = 42
    ): Pair<Matrix, Vector> = withContext(Dispatchers.Default) {
        
        val random = Random(seed)
        
        // Generate random true weights
        val trueWeights = List(features) { random.nextGaussian() }
        
        val X = mutableListOf<List<Double>>()
        val y = mutableListOf<Double>()
        
        repeat(samples) {
            val sample = mutableListOf<Double>()
            var target = 0.0
            
            repeat(features) { j ->
                val featureValue = random.nextGaussian()
                sample.add(featureValue)
                target += trueWeights[j] * featureValue
            }
            
            // Add noise
            target += random.nextGaussian() * noiseLevel
            
            X.add(sample)
            y.add(target)
        }
        
        Pair(X, y)
    }
    
    /**
     * Train-test split with proper randomization
     */
    suspend fun trainTestSplit(
        features: Matrix,
        targets: Vector,
        testRatio: Double = 0.2,
        seed: Int = 42
    ): TrainTestSplit = withContext(Dispatchers.Default) {
        
        require(features.size == targets.size) { "Features and targets must have same number of samples" }
        require(testRatio in 0.0..1.0) { "Test ratio must be between 0 and 1" }
        
        val totalSamples = features.size
        val testSize = (totalSamples * testRatio).toInt()
        val trainSize = totalSamples - testSize
        
        // Create and shuffle indices
        val indices = (0 until totalSamples).shuffled(Random(seed))
        
        val trainIndices = indices.take(trainSize)
        val testIndices = indices.drop(trainSize)
        
        val trainFeatures = trainIndices.map { features[it] }
        val testFeatures = testIndices.map { features[it] }
        val trainTargets = trainIndices.map { targets[it] }
        val testTargets = testIndices.map { targets[it] }
        
        TrainTestSplit(trainFeatures, testFeatures, trainTargets, testTargets)
    }
    
    data class TrainTestSplit(
        val trainFeatures: Matrix,
        val testFeatures: Matrix,
        val trainTargets: Vector,
        val testTargets: Vector
    )
}

// Extension function for Random.nextGaussian()
fun Random.nextGaussian(): Double {
    return sqrt(-2.0 * ln(nextDouble())) * cos(2.0 * PI * nextDouble())
}

// MARK: - Data Validation

class EnterpriseDataValidator(
    private val minValue: Double = -1e9,
    private val maxValue: Double = 1e9,
    private val allowMissing: Boolean = false,
    private val maxMissingRatio: Double = 0.1,
    private val logger: MLLogger = AsyncConsoleLogger()
) {
    
    suspend fun validate(features: Matrix, targets: Vector? = null): ValidationResult {
        val monitor = PerformanceMonitor("Data Validation", logger)
        
        return try {
            val errors = mutableListOf<String>()
            val warnings = mutableListOf<String>()
            val atomicMissingValues = AtomicInteger(0)
            val featureMissingCounts = ConcurrentHashMap<String, Int>()
            val featureStatistics = ConcurrentHashMap<String, FeatureStatistics>()
            
            val totalSamples = features.size
            val totalFeatures = features.firstOrNull()?.size ?: 0
            
            if (totalSamples == 0 || totalFeatures == 0) {
                errors.add("Empty dataset provided")
                return ValidationResult(
                    isValid = false, errors = errors, warnings = warnings,
                    totalSamples = totalSamples, totalFeatures = totalFeatures,
                    missingValues = 0, missingValueRatio = 0.0
                )
            }
            
            // Validate feature matrix
            validateFeatures(
                features, errors, warnings, atomicMissingValues,
                featureMissingCounts, featureStatistics
            )
            
            // Validate targets if provided
            targets?.let {
                validateTargets(features, it, errors)
            }
            
            // Calculate missing value ratio
            val missingValues = atomicMissingValues.get()
            val totalValues = totalSamples * totalFeatures
            val missingValueRatio = if (totalValues > 0) missingValues.toDouble() / totalValues else 0.0
            
            if (missingValueRatio > maxMissingRatio) {
                errors.add("Missing value ratio ${String.format("%.2f", missingValueRatio * 100)}% exceeds maximum allowed ${String.format("%.2f", maxMissingRatio * 100)}%")
            }
            
            val isValid = errors.isEmpty()
            
            logger.log(LogLevel.INFO, "Data validation completed: $totalSamples samples, $missingValues missing values, Valid: $isValid")
            
            ValidationResult(
                isValid = isValid, errors = errors, warnings = warnings,
                totalSamples = totalSamples, totalFeatures = totalFeatures,
                missingValues = missingValues, missingValueRatio = missingValueRatio,
                featureMissingCounts = featureMissingCounts.toMap(),
                featureStatistics = featureStatistics.toMap()
            )
            
        } finally {
            monitor.dispose()
        }
    }
    
    private suspend fun validateFeatures(
        features: Matrix,
        errors: MutableList<String>,
        warnings: MutableList<String>,
        atomicMissingValues: AtomicInteger,
        featureMissingCounts: ConcurrentHashMap<String, Int>,
        featureStatistics: ConcurrentHashMap<String, FeatureStatistics>
    ) = coroutineScope {
        
        val totalFeatures = features.firstOrNull()?.size ?: 0
        
        // Process features in parallel
        for (j in 0 until totalFeatures) {
            launch {
                val featureName = "feature_$j"
                var localMissingCount = 0
                val featureValues = mutableListOf<Double>()
                val localWarnings = mutableListOf<String>()
                
                for ((i, sample) in features.withIndex()) {
                    if (j >= sample.size) continue
                    
                    val value = sample[j]
                    
                    if (value.isNaN() || value.isInfinite()) {
                        localMissingCount++
                        if (!allowMissing) {
                            localWarnings.add("Invalid value at row $i, feature $j")
                        }
                    } else {
                        featureValues.add(value)
                        if (value < minValue || value > maxValue) {
                            localWarnings.add("Value ${String.format("%.4f", value)} at row $i, feature $j outside expected range [$minValue, $maxValue]")
                        }
                    }
                }
                
                // Update shared collections safely
                atomicMissingValues.addAndGet(localMissingCount)
                warnings.addAll(localWarnings)
                
                if (localMissingCount > 0) {
                    featureMissingCounts[featureName] = localMissingCount
                }
                
                if (featureValues.isNotEmpty()) {
                    featureStatistics[featureName] = MathUtils.calculateStatistics(featureValues)
                }
            }
        }
    }
    
    private fun validateTargets(features: Matrix, targets: Vector, errors: MutableList<String>) {
        if (features.size != targets.size) {
            errors.add("Feature matrix rows must match target vector length: ${features.size} != ${targets.size}")
        }
        
        val invalidTargets = targets.count { it.isNaN() || it.isInfinite() }
        if (invalidTargets > 0) {
            errors.add("Found $invalidTargets invalid target values")
        }
    }
}

// MARK: - Feature Engineering

class AdvancedFeatureEngineer(private val logger: MLLogger = AsyncConsoleLogger()) {
    
    private val transformCache = ConcurrentHashMap<String, FeatureTransformResult>()
    
    /**
     * Create polynomial features with caching
     */
    suspend fun createPolynomialFeatures(features: Matrix, degree: Int = 2): FeatureTransformResult {
        val monitor = PerformanceMonitor("Polynomial Feature Creation", logger)
        
        return try {
            require(degree >= 1) { "Polynomial degree must be >= 1" }
            
            val cacheKey = "poly_${features.size}_${features.firstOrNull()?.size ?: 0}_$degree"
            
            transformCache[cacheKey]?.let { cached ->
                logger.log(LogLevel.DEBUG, "Using cached polynomial features")
                return cached
            }
            
            withContext(Dispatchers.Default) {
                val samples = features.size
                val originalFeatures = features.firstOrNull()?.size ?: 0
                
                // Calculate total number of polynomial features
                var newFeatureCount = originalFeatures
                for (d in 2..degree) {
                    newFeatureCount += combinationCount(originalFeatures, d)
                }
                
                val result = MutableList(samples) { MutableList(newFeatureCount) { 0.0 } }
                
                // Copy original features
                for (i in 0 until samples) {
                    for (j in 0 until originalFeatures) {
                        result[i][j] = features[i][j]
                    }
                }
                
                // Generate polynomial combinations
                var featureIdx = originalFeatures
                
                for (d in 2..degree) {
                    val combinations = generateCombinations(originalFeatures, d)
                    
                    for (combo in combinations) {
                        for (i in 0 until samples) {
                            var value = 1.0
                            for (feature in combo) {
                                value *= features[i][feature]
                            }
                            result[i][featureIdx] = value
                        }
                        featureIdx++
                    }
                }
                
                val transformResult = FeatureTransformResult(
                    transformedFeatures = result,
                    transformationParameters = mapOf(
                        "degree" to degree,
                        "originalFeatures" to originalFeatures,
                        "newFeatures" to newFeatureCount
                    )
                )
                
                transformCache[cacheKey] = transformResult
                transformResult
            }
        } finally {
            monitor.dispose()
        }
    }
    
    /**
     * Standardize features with async processing
     */
    suspend fun standardizeFeatures(features: Matrix): FeatureTransformResult {
        val monitor = PerformanceMonitor("Feature Standardization", logger)
        
        return try {
            withContext(Dispatchers.Default) {
                val samples = features.size
                val featureCount = features.firstOrNull()?.size ?: 0
                
                val means = MutableList(featureCount) { 0.0 }
                val stds = MutableList(featureCount) { 0.0 }
                
                // Calculate means in parallel
                coroutineScope {
                    for (j in 0 until featureCount) {
                        launch {
                            val sum = features.sumOf { it[j] }
                            means[j] = sum / samples
                        }
                    }
                }
                
                // Calculate standard deviations in parallel
                coroutineScope {
                    for (j in 0 until featureCount) {
                        launch {
                            val sumSq = features.sumOf { (it[j] - means[j]).pow(2) }
                            stds[j] = sqrt(sumSq / (samples - 1))
                            
                            // Prevent division by zero
                            if (stds[j] < 1e-10) {
                                stds[j] = 1.0
                            }
                        }
                    }
                }
                
                // Apply standardization
                val result = features.map { sample ->
                    sample.mapIndexed { j, value ->
                        (value - means[j]) / stds[j]
                    }
                }
                
                FeatureTransformResult(
                    transformedFeatures = result,
                    featureMeans = means,
                    featureStds = stds,
                    transformationParameters = mapOf(
                        "method" to "standardization",
                        "samples" to samples,
                        "features" to featureCount
                    )
                )
            }
        } finally {
            monitor.dispose()
        }
    }
    
    private fun combinationCount(n: Int, k: Int): Int {
        if (k > n) return 0
        if (k == 0 || k == n) return 1
        
        var result = 1
        for (i in 0 until minOf(k, n - k)) {
            result = result * (n - i) / (i + 1)
        }
        return result
    }
    
    private fun generateCombinations(n: Int, k: Int): List<List<Int>> {
        val combinations = mutableListOf<List<Int>>()
        val combo = MutableList(k) { 0 }
        
        fun generate(start: Int, depth: Int) {
            if (depth == k) {
                combinations.add(combo.toList())
                return
            }
            
            for (i in start until n) {
                combo[depth] = i
                generate(i, depth + 1)
            }
        }
        
        generate(0, 0)
        return combinations
    }
}

// MARK: - Machine Learning Model Interface

interface MLModel {
    val isTrained: Boolean
    suspend fun train(features: Matrix, targets: Vector)
    suspend fun predict(features: Matrix): Vector
    suspend fun evaluate(features: Matrix, targets: Vector): ModelMetrics
    suspend fun save(filePath: String)
    suspend fun load(filePath: String)
}

// MARK: - Linear Regression Implementation

class EnterpriseLinearRegression(
    private val config: TrainingConfig = TrainingConfig(),
    private val logger: MLLogger = AsyncConsoleLogger()
) : MLModel {
    
    private var weights: MutableVector = mutableListOf()
    private var bias: Double = 0.0
    private val _isTrained = AtomicBoolean(false)
    private val modelMutex = Mutex()
    
    // Training statistics
    private var trainingHistory: MutableList<Double> = mutableListOf()
    private var lastTrainingTime: Long = 0L
    private var iterationsCompleted: Int = 0
    
    override val isTrained: Boolean
        get() = _isTrained.get()
    
    override suspend fun train(features: Matrix, targets: Vector) {
        val monitor = PerformanceMonitor("Linear Regression Training", logger)
        
        return try {
            modelMutex.withLock {
                require(features.size == targets.size) { 
                    "Feature matrix rows must match target vector size: ${features.size} != ${targets.size}" 
                }
                require(features.isNotEmpty() && features.first().isNotEmpty()) { 
                    "Empty dataset provided for training" 
                }
                
                val samples = features.size
                val featureCount = features.first().size
                
                // Initialize parameters
                weights = MutableList(featureCount) { 0.0 }
                bias = 0.0
                trainingHistory.clear()
                iterationsCompleted = 0
                
                val startTime = System.currentTimeMillis()
                
                // Training with gradient descent
                performGradientDescent(features, targets)
                
                lastTrainingTime = System.currentTimeMillis() - startTime
                _isTrained.set(true)
                
                logger.log(LogLevel.INFO, "Linear regression training completed")
            }
        } finally {
            monitor.dispose()
        }
    }
    
    override suspend fun predict(features: Matrix): Vector {
        require(isTrained) { "Model must be trained before making predictions" }
        require(features.first().size == weights.size) { 
            "Feature count mismatch: expected ${weights.size}, got ${features.first().size}" 
        }
        
        return computePredictions(features)
    }
    
    override suspend fun evaluate(features: Matrix, targets: Vector): ModelMetrics {
        val predictionStart = System.currentTimeMillis()
        val predictions = predict(features)
        val predictionTime = System.currentTimeMillis() - predictionStart
        
        return withContext(Dispatchers.Default) {
            // Calculate metrics in parallel
            val mse = predictions.zip(targets) { p, t -> (p - t).pow(2) }.average()
            val mae = predictions.zip(targets) { p, t -> abs(p - t) }.average()
            
            val meanTarget = targets.average()
            val totalSumSquares = targets.sumOf { (it - meanTarget).pow(2) }
            val residualSumSquares = predictions.zip(targets) { p, t -> (p - t).pow(2) }.sum()
            val rSquared = if (totalSumSquares > 1e-10) 1 - (residualSumSquares / totalSumSquares) else 0.0
            
            ModelMetrics(
                mse = mse,
                rmse = sqrt(mse),
                mae = mae,
                rSquared = rSquared,
                trainingTime = lastTrainingTime,
                predictionTime = predictionTime,
                iterationsCompleted = iterationsCompleted,
                convergenceValue = trainingHistory.lastOrNull() ?: 0.0,
                trainingHistory = trainingHistory.toList()
            )
        }
    }
    
    override suspend fun save(filePath: String) {
        require(isTrained) { "Cannot save untrained model" }
        
        val modelData = ModelData(
            weights = weights.toList(),
            bias = bias,
            config = config,
            trainingHistory = trainingHistory.toList(),
            trainingTime = lastTrainingTime,
            iterationsCompleted = iterationsCompleted
        )
        
        val jsonString = Json.encodeToString(modelData)
        File(filePath).writeText(jsonString)
        
        logger.log(LogLevel.INFO, "Model saved to $filePath")
    }
    
    override suspend fun load(filePath: String) {
        val jsonString = File(filePath).readText()
        val modelData = Json.decodeFromString<ModelData>(jsonString)
        
        modelMutex.withLock {
            weights = modelData.weights.toMutableList()
            bias = modelData.bias
            trainingHistory = modelData.trainingHistory.toMutableList()
            iterationsCompleted = modelData.iterationsCompleted
            _isTrained.set(true)
        }
        
        logger.log(LogLevel.INFO, "Model loaded from $filePath")
    }
    
    // MARK: - Private Training Methods
    
    private suspend fun performGradientDescent(features: Matrix, targets: Vector) {
        var prevCost = Double.MAX_VALUE
        var patienceCounter = 0
        
        for (iteration in 0 until config.maxIterations) {
            // Forward pass
            val predictions = computePredictions(features)
            
            // Compute cost
            val cost = computeCost(predictions, targets)
            trainingHistory.add(cost)
            
            // Check convergence
            if (abs(prevCost - cost) < config.convergenceThreshold) {
                logger.log(LogLevel.INFO, "Convergence achieved at iteration $iteration")
                break
            }
            
            // Early stopping check
            if (config.enableEarlyStopping) {
                if (cost > prevCost) {
                    patienceCounter++
                    if (patienceCounter >= config.earlyStoppingPatience) {
                        logger.log(LogLevel.INFO, "Early stopping at iteration $iteration")
                        break
                    }
                } else {
                    patienceCounter = 0
                }
            }
            
            prevCost = cost
            
            // Backward pass
            updateParameters(features, predictions, targets)
            iterationsCompleted = iteration + 1
        }
    }
    
    private suspend fun computePredictions(features: Matrix): Vector = withContext(Dispatchers.Default) {
        features.map { sample ->
            bias + sample.zip(weights) { x, w -> x * w }.sum()
        }
    }
    
    private suspend fun computeCost(predictions: Vector, targets: Vector): Double = withContext(Dispatchers.Default) {
        var cost = predictions.zip(targets) { p, t -> (p - t).pow(2) }.sum() / (2.0 * targets.size)
        
        // Add regularization if enabled
        if (config.enableRegularization) {
            val regularization = config.regularizationStrength * weights.sumOf { it * it }
            cost += regularization
        }
        
        cost
    }
    
    private suspend fun updateParameters(features: Matrix, predictions: Vector, targets: Vector) {
        val samples = features.size.toDouble()
        val featureCount = weights.size
        
        // Compute gradients
        val weightGradients = MutableList(featureCount) { 0.0 }
        var biasGradient = 0.0
        
        for (i in features.indices) {
            val error = predictions[i] - targets[i]
            biasGradient += error
            
            for (j in 0 until featureCount) {
                weightGradients[j] += error * features[i][j]
            }
        }
        
        // Update parameters
        bias -= config.learningRate * biasGradient / samples
        
        for (j in 0 until featureCount) {
            var gradient = weightGradients[j] / samples
            
            // Add regularization gradient if enabled
            if (config.enableRegularization) {
                gradient += config.regularizationStrength * weights[j]
            }
            
            weights[j] -= config.learningRate * gradient
        }
    }
    
    @Serializable
    private data class ModelData(
        val weights: List<Double>,
        val bias: Double,
        val config: TrainingConfig,
        val trainingHistory: List<Double>,
        val trainingTime: Long,
        val iterationsCompleted: Int
    )
}

// MARK: - Production ML Pipeline

class EnterpriseMLPipeline(
    private val model: MLModel = EnterpriseLinearRegression(),
    private val validator: EnterpriseDataValidator = EnterpriseDataValidator(),
    private val logger: MLLogger = AsyncConsoleLogger()
) {
    private val featureEngineer = AdvancedFeatureEngineer(logger)
    private val pipelineMutex = Mutex()
    
    private var lastTransformation: FeatureTransformResult? = null
    private var isStandardized = false
    private val _isTraining = AtomicBoolean(false)
    
    val isTraining: Boolean
        get() = _isTraining.get()
    
    /**
     * Train the complete ML pipeline
     */
    suspend fun train(features: Matrix, targets: Vector, validationSplit: Double = 0.2) {
        val monitor = PerformanceMonitor("Enterprise Pipeline Training", logger)
        
        return try {
            require(!isTraining) { "Pipeline training already in progress" }
            
            _isTraining.set(true)
            
            pipelineMutex.withLock {
                // Data validation
                logger.log(LogLevel.INFO, "Starting data validation...")
                val validation = validator.validate(features, targets)
                
                if (!validation.isValid) {
                    val errorMsg = "Data validation failed: ${validation.errors.joinToString("; ")}"
                    throw DataValidationException(errorMsg, validation.errors)
                }
                
                // Feature standardization
                logger.log(LogLevel.INFO, "Applying feature standardization...")
                lastTransformation = featureEngineer.standardizeFeatures(features)
                isStandardized = true
                
                // Train-validation split
                val splitData = MathUtils.trainTestSplit(
                    lastTransformation!!.transformedFeatures,
                    targets,
                    validationSplit
                )
                
                // Model training
                logger.log(LogLevel.INFO, "Starting model training...")
                model.train(splitData.trainFeatures, splitData.trainTargets)
                
                // Validation evaluation
                if (validationSplit > 0) {
                    logger.log(LogLevel.INFO, "Evaluating on validation set...")
                    val metrics = model.evaluate(splitData.testFeatures, splitData.testTargets)
                    logger.log(LogLevel.INFO, "Validation R¬≤: ${String.format("%.4f", metrics.rSquared)}, RMSE: ${String.format("%.4f", metrics.rmse)}")
                }
                
                logger.log(LogLevel.INFO, "Pipeline training completed successfully")
            }
        } finally {
            _isTraining.set(false)
            monitor.dispose()
        }
    }
    
    /**
     * Make predictions using the trained pipeline
     */
    suspend fun predict(features: Matrix): Vector {
        require(model.isTrained) { "Pipeline must be trained before making predictions" }
        
        return try {
            var processedFeatures = features
            
            // Apply same transformation as training
            if (isStandardized && lastTransformation != null) {
                processedFeatures = applyStandardization(features, lastTransformation!!)
            }
            
            model.predict(processedFeatures)
            
        } catch (error: Throwable) {
            logger.logException(error, "Pipeline prediction failed")
            throw error
        }
    }
    
    /**
     * Evaluate the pipeline performance
     */
    suspend fun evaluate(features: Matrix, targets: Vector): ModelMetrics {
        return try {
            var processedFeatures = features
            
            // Apply same transformation as training
            if (isStandardized && lastTransformation != null) {
                processedFeatures = applyStandardization(features, lastTransformation!!)
            }
            
            model.evaluate(processedFeatures, targets)
            
        } catch (error: Throwable) {
            logger.logException(error, "Pipeline evaluation failed")
            throw error
        }
    }
    
    /**
     * Save the complete pipeline
     */
    suspend fun savePipeline(directoryPath: String) {
        val dir = File(directoryPath)
        if (!dir.exists()) {
            dir.mkdirs()
        }
        
        // Save model
        model.save("$directoryPath/model.json")
        
        // Save feature transformation parameters
        lastTransformation?.let { transformation ->
            val transformData = mapOf(
                "isStandardized" to isStandardized,
                "featureMeans" to transformation.featureMeans,
                "featureStds" to transformation.featureStds,
                "transformationParameters" to transformation.transformationParameters
            )
            
            val jsonString = Json.encodeToString(transformData)
            File("$directoryPath/feature_transform.json").writeText(jsonString)
        }
        
        logger.log(LogLevel.INFO, "Pipeline saved to $directoryPath")
    }
    
    // MARK: - Private Methods
    
    private suspend fun applyStandardization(features: Matrix, transformation: FeatureTransformResult): Matrix {
        val means = transformation.featureMeans ?: return features
        val stds = transformation.featureStds ?: return features
        
        return withContext(Dispatchers.Default) {
            features.map { sample ->
                sample.zip(means.zip(stds)) { value, (mean, std) ->
                    (value - mean) / std
                }
            }
        }
    }
    
    // MARK: - Pipeline Status
    
    data class PipelineStatus(
        val isModelTrained: Boolean,
        val isStandardized: Boolean,
        val isTraining: Boolean
    )
    
    val pipelineStatus: PipelineStatus
        get() = PipelineStatus(model.isTrained, isStandardized, isTraining)
}

// MARK: - Demonstration Function

/**
 * Comprehensive demonstration of Kotlin ML patterns
 */
suspend fun demonstrateKotlinMLPatterns() {
    val logger = AsyncConsoleLogger()
    
    try {
        logger.log(LogLevel.INFO, "üöÄ Kotlin ML Production Patterns Demonstration")
        logger.log(LogLevel.INFO, "===============================================")
        
        // Generate synthetic dataset
        logger.log(LogLevel.INFO, "üìä Generating synthetic dataset...")
        val (features, targets) = MathUtils.generateRegressionDataset(1000, 5, 0.1)
        
        // Create enterprise pipeline
        logger.log(LogLevel.INFO, "üèóÔ∏è Creating enterprise ML pipeline...")
        val config = TrainingConfig(
            learningRate = 0.01,
            maxIterations = 1000,
            convergenceThreshold = 1e-6,
            validationSplit = 0.2,
            enableEarlyStopping = true,
            earlyStoppingPatience = 10
        )
        
        val pipeline = EnterpriseMLPipeline(
            model = EnterpriseLinearRegression(config, logger),
            validator = EnterpriseDataValidator(logger = logger),
            logger = logger
        )
        
        // Train pipeline
        logger.log(LogLevel.INFO, "üîÑ Training production ML pipeline...")
        pipeline.train(features, targets, 0.2)
        logger.log(LogLevel.INFO, "‚úÖ Model training completed")
        
        // Make predictions
        logger.log(LogLevel.INFO, "üîÆ Making predictions...")
        val (testFeatures, testTargets) = MathUtils.generateRegressionDataset(100, 5, 0.1, 123)
        val predictions = pipeline.predict(testFeatures)
        
        val samplePredictions = predictions.take(5).joinToString(", ") { "%.4f".format(it) }
        logger.log(LogLevel.INFO, "Sample predictions: $samplePredictions")
        
        // Model evaluation
        logger.log(LogLevel.INFO, "üìä Evaluating model performance...")
        val metrics = pipeline.evaluate(testFeatures, testTargets)
        
        logger.log(LogLevel.INFO, "R¬≤ Score: ${"%.4f".format(metrics.rSquared)}")
        logger.log(LogLevel.INFO, "RMSE: ${"%.4f".format(metrics.rmse)}")
        logger.log(LogLevel.INFO, "MAE: ${"%.4f".format(metrics.mae)}")
        logger.log(LogLevel.INFO, "Training Time: ${"%.2f".format(metrics.trainingTime / 1000.0)} seconds")
        logger.log(LogLevel.INFO, "Prediction Time: ${metrics.predictionTime}ms")
        
        // Feature engineering demonstration
        logger.log(LogLevel.INFO, "üîß Feature Engineering demonstration...")
        val featureEngineer = AdvancedFeatureEngineer(logger)
        val polynomialResult = featureEngineer.createPolynomialFeatures(testFeatures, 2)
        
        logger.log(LogLevel.INFO, "Original features: ${testFeatures.first().size}, Polynomial features: ${polynomialResult.transformedFeatures.first().size}")
        
        // Performance monitoring summary
        logger.log(LogLevel.INFO, "‚ö° Performance characteristics:")
        logger.log(LogLevel.INFO, "- Coroutines: ‚úÖ Structured concurrency with async/await")
        logger.log(LogLevel.INFO, "- Type safety: ‚úÖ Sealed classes and data classes")
        logger.log(LogLevel.INFO, "- Null safety: ‚úÖ Kotlin null safety system")
        logger.log(LogLevel.INFO, "- Multiplatform: ‚úÖ JVM, Android, Native compatibility")
        logger.log(LogLevel.INFO, "- Memory management: ‚úÖ JVM GC with efficient collections")
        
        logger.log(LogLevel.INFO, "‚úÖ Kotlin ML demonstration completed successfully!")
        
    } catch (error: Throwable) {
        logger.logException(error, "Fatal error during demonstration")
        throw error
    } finally {
        (logger as? AsyncConsoleLogger)?.close()
    }
}

// MARK: - Main Entry Point

suspend fun main() {
    demonstrateKotlinMLPatterns()
}