/*
Production-Ready Machine Learning Patterns in Scala
==================================================

This module demonstrates industry-standard ML patterns in Scala with proper
functional programming, Actor model integration, and production deployment
considerations for AI training datasets.

Key Features:
- Functional programming with immutable data structures
- Actor model with Akka integration patterns
- Scala collections with lazy evaluation and parallel processing
- Type safety with case classes and pattern matching
- SBT build integration and artifact management
- Comprehensive error handling with Try and Either monads
- Extensive documentation for AI learning
- Production-ready patterns with streaming and reactive programming

Author: AI Training Dataset
License: MIT
*/

package ml.production.patterns

import scala.concurrent.{Future, ExecutionContext}
import scala.util.{Try, Success, Failure, Random}
import scala.collection.parallel.CollectionConverters._
import scala.collection.mutable
import scala.math._
import java.time.{Instant, Duration}
import java.io.{File, PrintWriter}
import java.nio.file.{Files, Paths}
import spray.json._
import DefaultJsonProtocol._

// MARK: - Type Aliases and ADTs

type Matrix = Vector[Vector[Double]]
type Vec = Vector[Double]

// MARK: - Error Hierarchy using Sealed Traits

sealed abstract class MLError(message: String, val context: String) extends Exception(s"ML Error: $message")

case class DataValidationError(message: String, validationErrors: List[String] = List.empty) 
  extends MLError(s"Data Validation - $message", "data_validation")

case class ModelTrainingError(message: String, iterationsFailed: Option[Int] = None) 
  extends MLError(s"Model Training - $message", "model_training")

case class ModelPredictionError(message: String) 
  extends MLError(s"Model Prediction - $message", "model_prediction")

case class FeatureEngineeringError(message: String) 
  extends MLError(s"Feature Engineering - $message", "feature_engineering")

// MARK: - Data Classes using Case Classes

case class ValidationResult(
  isValid: Boolean,
  errors: List[String] = List.empty,
  warnings: List[String] = List.empty,
  totalSamples: Int = 0,
  totalFeatures: Int = 0,
  missingValues: Int = 0,
  missingValueRatio: Double = 0.0,
  featureMissingCounts: Map[String, Int] = Map.empty,
  featureStatistics: Map[String, FeatureStatistics] = Map.empty
)

case class FeatureStatistics(
  min: Double,
  max: Double,
  mean: Double,
  standardDeviation: Double,
  variance: Double,
  skewness: Double,
  kurtosis: Double
)

case class ModelMetrics(
  mse: Double,
  rmse: Double,
  mae: Double,
  rSquared: Double,
  trainingTime: Double,
  predictionTime: Double,
  iterationsCompleted: Int,
  convergenceValue: Double,
  trainingHistory: List[Double]
)

case class TrainingConfig(
  learningRate: Double = 0.01,
  maxIterations: Int = 1000,
  convergenceThreshold: Double = 1e-6,
  validationSplit: Double = 0.2,
  enableEarlyStopping: Boolean = true,
  earlyStoppingPatience: Int = 10,
  enableRegularization: Boolean = false,
  regularizationStrength: Double = 0.01,
  batchSize: Int = 32
) {
  require(learningRate > 0, "Learning rate must be positive")
  require(maxIterations > 0, "Max iterations must be positive")
  require(convergenceThreshold > 0, "Convergence threshold must be positive")
  require(validationSplit >= 0 && validationSplit <= 1, "Validation split must be between 0 and 1")
  require(earlyStoppingPatience > 0, "Early stopping patience must be positive")
  require(regularizationStrength >= 0, "Regularization strength must be non-negative")
  require(batchSize > 0, "Batch size must be positive")
}

case class FeatureTransformResult(
  transformedFeatures: Matrix,
  featureMeans: Option[Vec] = None,
  featureStds: Option[Vec] = None,
  transformationParameters: Map[String, Any] = Map.empty
)

// MARK: - JSON Protocol for Serialization

object MLJsonProtocol extends DefaultJsonProtocol {
  implicit val featureStatisticsFormat = jsonFormat7(FeatureStatistics)
  implicit val validationResultFormat = jsonFormat9(ValidationResult)
  implicit val modelMetricsFormat = jsonFormat9(ModelMetrics)
  implicit val trainingConfigFormat = jsonFormat9(TrainingConfig)
}

import MLJsonProtocol._

// MARK: - Logging Trait

sealed trait LogLevel
object LogLevel {
  case object Debug extends LogLevel
  case object Info extends LogLevel
  case object Warning extends LogLevel
  case object Error extends LogLevel
  case object Critical extends LogLevel
  
  def toString(level: LogLevel): String = level match {
    case Debug => "DEBUG"
    case Info => "INFO"
    case Warning => "WARNING"
    case Error => "ERROR"
    case Critical => "CRITICAL"
  }
}

trait MLLogger {
  def log(level: LogLevel, message: String): Unit
  def logException(error: Throwable, context: String): Unit
  
  // Convenience methods
  def debug(message: String): Unit = log(LogLevel.Debug, message)
  def info(message: String): Unit = log(LogLevel.Info, message)
  def warning(message: String): Unit = log(LogLevel.Warning, message)
  def error(message: String): Unit = log(LogLevel.Error, message)
}

class ConsoleLogger extends MLLogger {
  private val logBuffer = mutable.Queue[String]()
  private val maxBufferSize = 1000
  
  def log(level: LogLevel, message: String): Unit = {
    val timestamp = Instant.now().toString
    val levelStr = LogLevel.toString(level)
    val logEntry = s"[$timestamp] [$levelStr] $message"
    
    // Thread-safe logging with buffer management
    synchronized {
      logBuffer.enqueue(logEntry)
      if (logBuffer.size > maxBufferSize) {
        logBuffer.dequeue()
      }
      println(logEntry)
    }
  }
  
  def logException(error: Throwable, context: String): Unit = {
    error(s"$context: ${error.getMessage}")
    error match {
      case mlError: MLError =>
        debug(s"ML Error Context: ${mlError.context}")
      case _ =>
    }
    debug(s"Stack Trace: ${error.getStackTrace.mkString("\n")}")
  }
  
  def getLogBuffer: List[String] = synchronized(logBuffer.toList)
  def clearLogBuffer(): Unit = synchronized(logBuffer.clear())
}

// MARK: - Performance Monitoring

class PerformanceMonitor(operationName: String, logger: MLLogger) {
  private val startTime = System.nanoTime()
  
  def dispose(): Unit = {
    val endTime = System.nanoTime()
    val duration = (endTime - startTime) / 1_000_000.0 // Convert to milliseconds
    logger.info(f"[PERFORMANCE] $operationName completed in ${duration}%.2fms")
  }
  
  def elapsedTime: Double = (System.nanoTime() - startTime) / 1_000_000_000.0 // Convert to seconds
}

object PerformanceMonitor {
  def timed[T](operationName: String, logger: MLLogger)(operation: => T): T = {
    val monitor = new PerformanceMonitor(operationName, logger)
    try {
      operation
    } finally {
      monitor.dispose()
    }
  }
}

// MARK: - Mathematical Utilities

object MathUtils {
  
  /**
   * Matrix multiplication using parallel collections for performance
   */
  def matrixMultiply(a: Matrix, b: Matrix): Matrix = {
    require(a.nonEmpty && b.nonEmpty, "Matrices cannot be empty")
    require(a.head.size == b.size, s"Matrix dimensions don't match for multiplication: ${a.head.size} != ${b.size}")
    
    val rowsA = a.size
    val colsA = a.head.size
    val colsB = b.head.size
    
    // Use parallel collections for performance
    (0 until rowsA).par.map { i =>
      (0 until colsB).map { j =>
        (0 until colsA).map(k => a(i)(k) * b(k)(j)).sum
      }.toVector
    }.toVector
  }
  
  /**
   * Vectorized dot product calculation
   */
  def dotProduct(a: Vec, b: Vec): Double = {
    require(a.size == b.size, "Vector lengths must match")
    a.zip(b).map { case (x, y) => x * y }.sum
  }
  
  /**
   * Calculate comprehensive statistics for a vector using functional approach
   */
  def calculateStatistics(values: Vec): FeatureStatistics = {
    val validValues = values.filter(v => v.isFinite && !v.isNaN)
    
    if (validValues.isEmpty) {
      return FeatureStatistics(
        min = Double.NaN, max = Double.NaN, mean = Double.NaN,
        standardDeviation = Double.NaN, variance = Double.NaN,
        skewness = Double.NaN, kurtosis = Double.NaN
      )
    }
    
    val count = validValues.size.toDouble
    val min = validValues.min
    val max = validValues.max
    val mean = validValues.sum / count
    
    // Functional approach to calculate higher moments
    val deviations = validValues.map(_ - mean)
    val squaredDeviations = deviations.map(d => d * d)
    val variance = squaredDeviations.sum / (count - 1)
    val standardDeviation = sqrt(variance)
    
    // Calculate skewness and kurtosis
    val normalizedDeviations = deviations.map(_ / standardDeviation)
    val skewness = normalizedDeviations.map(d => d * d * d).sum / count
    val kurtosis = normalizedDeviations.map(d => d * d * d * d).sum / count - 3.0 // Excess kurtosis
    
    FeatureStatistics(
      min = min, max = max, mean = mean,
      standardDeviation = standardDeviation, variance = variance,
      skewness = skewness, kurtosis = kurtosis
    )
  }
  
  /**
   * Generate synthetic regression dataset using functional programming patterns
   */
  def generateRegressionDataset(samples: Int, features: Int, noiseLevel: Double = 0.1, seed: Int = 42): (Matrix, Vec) = {
    val rng = new Random(seed)
    
    // Generate true weights using functional approach
    val trueWeights = Vector.fill(features)(randomGaussian(rng))
    
    // Generate features and targets using comprehensions
    val (xData, yData) = (for {
      _ <- 0 until samples
      sample = Vector.fill(features)(randomGaussian(rng))
      target = dotProduct(sample, trueWeights) + randomGaussian(rng) * noiseLevel
    } yield (sample, target)).unzip
    
    (xData.toVector, yData.toVector)
  }
  
  /**
   * Train-test split using immutable collections
   */
  def trainTestSplit(features: Matrix, targets: Vec, testRatio: Double = 0.2, seed: Int = 42): (Matrix, Matrix, Vec, Vec) = {
    require(features.size == targets.size, "Features and targets must have same number of samples")
    require(testRatio >= 0 && testRatio <= 1, "Test ratio must be between 0 and 1")
    
    val totalSamples = features.size
    val testSize = (totalSamples * testRatio).toInt
    
    // Create shuffled indices using functional approach
    val rng = new Random(seed)
    val indices = rng.shuffle((0 until totalSamples).toList)
    
    val (trainIndices, testIndices) = indices.splitAt(totalSamples - testSize)
    
    // Use pattern matching and collection operations
    val trainFeatures = trainIndices.map(features)
    val testFeatures = testIndices.map(features)
    val trainTargets = trainIndices.map(targets)
    val testTargets = testIndices.map(targets)
    
    (trainFeatures.toVector, testFeatures.toVector, trainTargets.toVector, testTargets.toVector)
  }
  
  /**
   * Generate Gaussian random numbers using Box-Muller transform
   */
  private def randomGaussian(rng: Random): Double = {
    // Simple implementation for demonstration
    val u1 = rng.nextDouble()
    val u2 = rng.nextDouble()
    sqrt(-2.0 * log(u1)) * sin(2.0 * Pi * u2)
  }
}

// MARK: - Data Validation

class EnterpriseDataValidator(
  minValue: Double = -1e9,
  maxValue: Double = 1e9,
  allowMissing: Boolean = false,
  maxMissingRatio: Double = 0.1,
  logger: MLLogger = new ConsoleLogger
)(implicit ec: ExecutionContext) {
  
  /**
   * Validate features using Future for asynchronous processing
   */
  def validate(features: Matrix, targets: Option[Vec] = None): Future[ValidationResult] = {
    val monitor = new PerformanceMonitor("Data Validation", logger)
    
    Future {
      try {
        val totalSamples = features.size
        val totalFeatures = if (features.isEmpty) 0 else features.head.size
        
        if (totalSamples == 0 || totalFeatures == 0) {
          return ValidationResult(
            isValid = false,
            errors = List("Empty dataset provided"),
            totalSamples = totalSamples,
            totalFeatures = totalFeatures
          )
        }
        
        // Validate features using parallel processing
        val featureValidation = validateFeatures(features)
        
        // Validate targets if provided
        val targetErrors = targets match {
          case Some(tgts) => validateTargets(features, tgts)
          case None => List.empty
        }
        
        val allErrors = featureValidation.errors ++ targetErrors
        val missingValues = featureValidation.missingValues
        val totalValues = totalSamples * totalFeatures
        val missingValueRatio = if (totalValues > 0) missingValues.toDouble / totalValues else 0.0
        
        // Check missing value ratio
        val ratioErrors = if (missingValueRatio > maxMissingRatio) {
          List(f"Missing value ratio ${missingValueRatio * 100}%.2f%% exceeds maximum allowed ${maxMissingRatio * 100}%.2f%%")
        } else List.empty
        
        val finalErrors = allErrors ++ ratioErrors
        val isValid = finalErrors.isEmpty
        
        logger.info(s"Data validation completed: $totalSamples samples, $missingValues missing values, Valid: $isValid")
        
        ValidationResult(
          isValid = isValid,
          errors = finalErrors,
          warnings = featureValidation.warnings,
          totalSamples = totalSamples,
          totalFeatures = totalFeatures,
          missingValues = missingValues,
          missingValueRatio = missingValueRatio,
          featureMissingCounts = featureValidation.featureMissingCounts,
          featureStatistics = featureValidation.featureStatistics
        )
      } finally {
        monitor.dispose()
      }
    }
  }
  
  private case class FeatureValidationResult(
    errors: List[String],
    warnings: List[String],
    missingValues: Int,
    featureMissingCounts: Map[String, Int],
    featureStatistics: Map[String, FeatureStatistics]
  )
  
  private def validateFeatures(features: Matrix): FeatureValidationResult = {
    val totalFeatures = features.head.size
    
    // Process features in parallel using parallel collections
    val results = (0 until totalFeatures).par.map { j =>
      val featureName = s"feature_$j"
      val featureValues = mutable.ListBuffer[Double]()
      var localMissingCount = 0
      val localWarnings = mutable.ListBuffer[String]()
      
      features.zipWithIndex.foreach { case (sample, i) =>
        if (j < sample.size) {
          val value = sample(j)
          
          if (value.isNaN || value.isInfinite) {
            localMissingCount += 1
            if (!allowMissing) {
              localWarnings += s"Invalid value at row $i, feature $j"
            }
          } else {
            featureValues += value
            if (value < minValue || value > maxValue) {
              localWarnings += f"Value $value%.4f at row $i, feature $j outside expected range [$minValue, $maxValue]"
            }
          }
        }
      }
      
      val featureMissingCount = if (localMissingCount > 0) Map(featureName -> localMissingCount) else Map.empty
      val featureStats = if (featureValues.nonEmpty) {
        Map(featureName -> MathUtils.calculateStatistics(featureValues.toVector))
      } else Map.empty
      
      (localWarnings.toList, localMissingCount, featureMissingCount, featureStats)
    }.toList
    
    // Aggregate results using functional operations
    val (allWarnings, missingCounts, missingCountMaps, statsMaps) = results.unzip4
    
    FeatureValidationResult(
      errors = List.empty,
      warnings = allWarnings.flatten,
      missingValues = missingCounts.sum,
      featureMissingCounts = missingCountMaps.foldLeft(Map.empty[String, Int])(_ ++ _),
      featureStatistics = statsMaps.foldLeft(Map.empty[String, FeatureStatistics])(_ ++ _)
    )
  }
  
  private def validateTargets(features: Matrix, targets: Vec): List[String] = {
    val errors = mutable.ListBuffer[String]()
    
    if (features.size != targets.size) {
      errors += s"Feature matrix rows must match target vector length: ${features.size} != ${targets.size}"
    }
    
    val invalidTargets = targets.count(t => t.isNaN || t.isInfinite)
    if (invalidTargets > 0) {
      errors += s"Found $invalidTargets invalid target values"
    }
    
    errors.toList
  }
}

// MARK: - Feature Engineering

class AdvancedFeatureEngineer(logger: MLLogger = new ConsoleLogger)(implicit ec: ExecutionContext) {
  private val transformCache = mutable.Map[String, FeatureTransformResult]()
  
  /**
   * Create polynomial features using functional programming
   */
  def createPolynomialFeatures(features: Matrix, degree: Int = 2): Future[FeatureTransformResult] = {
    val monitor = new PerformanceMonitor("Polynomial Feature Creation", logger)
    
    Future {
      try {
        require(degree >= 1, "Polynomial degree must be >= 1")
        
        val cacheKey = s"poly_${features.size}_${features.headOption.map(_.size).getOrElse(0)}_$degree"
        
        transformCache.get(cacheKey) match {
          case Some(cached) =>
            logger.debug("Using cached polynomial features")
            cached
          case None =>
            val result = generatePolynomialFeatures(features, degree)
            transformCache(cacheKey) = result
            result
        }
      } finally {
        monitor.dispose()
      }
    }
  }
  
  /**
   * Standardize features using functional approach
   */
  def standardizeFeatures(features: Matrix): Future[FeatureTransformResult] = {
    val monitor = new PerformanceMonitor("Feature Standardization", logger)
    
    Future {
      try {
        val samples = features.size
        val featureCount = features.headOption.map(_.size).getOrElse(0)
        
        // Calculate means using parallel processing
        val means = (0 until featureCount).par.map { j =>
          features.map(_(j)).sum / samples.toDouble
        }.toVector
        
        // Calculate standard deviations using parallel processing
        val stds = (0 until featureCount).par.map { j =>
          val mean = means(j)
          val sumSq = features.map(sample => pow(sample(j) - mean, 2)).sum
          val std = sqrt(sumSq / (samples - 1).toDouble)
          if (std < 1e-10) 1.0 else std // Prevent division by zero
        }.toVector
        
        // Apply standardization using functional operations
        val result = features.map { sample =>
          sample.zipWithIndex.map { case (value, j) =>
            (value - means(j)) / stds(j)
          }
        }
        
        FeatureTransformResult(
          transformedFeatures = result,
          featureMeans = Some(means),
          featureStds = Some(stds),
          transformationParameters = Map(
            "method" -> "standardization",
            "samples" -> samples,
            "features" -> featureCount
          )
        )
      } finally {
        monitor.dispose()
      }
    }
  }
  
  private def generatePolynomialFeatures(features: Matrix, degree: Int): FeatureTransformResult = {
    val samples = features.size
    val originalFeatures = features.headOption.map(_.size).getOrElse(0)
    
    // Calculate total number of polynomial features using combinatorics
    val newFeatureCount = (1 to degree).map(d => combinationCount(originalFeatures, d)).sum
    
    val result = Array.ofDim[Double](samples, newFeatureCount)
    
    // Copy original features
    for {
      i <- features.indices
      j <- features(i).indices
    } {
      result(i)(j) = features(i)(j)
    }
    
    // Generate polynomial combinations using functional approach
    var featureIdx = originalFeatures
    
    for (d <- 2 to degree) {
      val combinations = generateCombinations(originalFeatures, d)
      
      combinations.foreach { combo =>
        for (i <- features.indices) {
          val value = combo.foldLeft(1.0)((acc, feature) => acc * features(i)(feature))
          result(i)(featureIdx) = value
        }
        featureIdx += 1
      }
    }
    
    FeatureTransformResult(
      transformedFeatures = result.map(_.toVector).toVector,
      transformationParameters = Map(
        "degree" -> degree,
        "originalFeatures" -> originalFeatures,
        "newFeatures" -> newFeatureCount
      )
    )
  }
  
  private def combinationCount(n: Int, k: Int): Int = {
    if (k > n) 0
    else if (k == 0 || k == n) 1
    else {
      var result = 1
      for (i <- 0 until math.min(k, n - k)) {
        result = result * (n - i) / (i + 1)
      }
      result
    }
  }
  
  private def generateCombinations(n: Int, k: Int): List[List[Int]] = {
    def combinations(list: List[Int], k: Int): List[List[Int]] = {
      if (k == 0) List(List.empty)
      else if (list.isEmpty) List.empty
      else {
        val head = list.head
        val tail = list.tail
        combinations(tail, k - 1).map(head :: _) ++ combinations(tail, k)
      }
    }
    
    combinations((0 until n).toList, k)
  }
}

// MARK: - Machine Learning Model Trait

trait MLModel {
  def isTrained: Boolean
  def train(features: Matrix, targets: Vec): Try[Unit]
  def predict(features: Matrix): Try[Vec]
  def evaluate(features: Matrix, targets: Vec): Try[ModelMetrics]
  def save(filePath: String): Try[Unit]
  def load(filePath: String): Try[Unit]
}

// MARK: - Linear Regression Implementation

class EnterpriseLinearRegression(
  config: TrainingConfig = TrainingConfig(),
  logger: MLLogger = new ConsoleLogger
)(implicit ec: ExecutionContext) extends MLModel {
  
  private var weights: Vec = Vector.empty
  private var bias: Double = 0.0
  private var _isTrained: Boolean = false
  
  // Training statistics
  private var trainingHistory: List[Double] = List.empty
  private var lastTrainingTime: Double = 0.0
  private var iterationsCompleted: Int = 0
  
  def isTrained: Boolean = _isTrained
  
  def train(features: Matrix, targets: Vec): Try[Unit] = {
    val monitor = new PerformanceMonitor("Linear Regression Training", logger)
    
    Try {
      require(features.size == targets.size, 
        s"Feature matrix rows must match target vector size: ${features.size} != ${targets.size}")
      require(features.nonEmpty && features.head.nonEmpty, "Empty dataset provided for training")
      
      val samples = features.size
      val featureCount = features.head.size
      
      // Initialize parameters
      weights = Vector.fill(featureCount)(0.0)
      bias = 0.0
      trainingHistory = List.empty
      iterationsCompleted = 0
      
      val startTime = System.nanoTime()
      
      // Training with gradient descent using functional approach
      performGradientDescent(features, targets)
      
      lastTrainingTime = (System.nanoTime() - startTime) / 1_000_000_000.0
      _isTrained = true
      
      logger.info("Linear regression training completed")
    }.recoverWith {
      case ex => 
        logger.logException(ex, "Training failed")
        Failure(ModelTrainingError(ex.getMessage))
    }.andThen(_ => monitor.dispose())
  }
  
  def predict(features: Matrix): Try[Vec] = {
    if (!isTrained) {
      return Failure(ModelPredictionError("Model must be trained before making predictions"))
    }
    
    if (features.headOption.exists(_.size != weights.size)) {
      return Failure(ModelPredictionError(
        s"Feature count mismatch: expected ${weights.size}, got ${features.headOption.map(_.size).getOrElse(0)}"
      ))
    }
    
    Try {
      computePredictions(features)
    }
  }
  
  def evaluate(features: Matrix, targets: Vec): Try[ModelMetrics] = {
    val predictionStart = System.nanoTime()
    
    predict(features).flatMap { predictions =>
      val predictionTime = (System.nanoTime() - predictionStart) / 1_000_000_000.0
      
      Try {
        // Calculate metrics using functional programming
        val errors = predictions.zip(targets).map { case (p, t) => p - t }
        val squaredErrors = errors.map(e => e * e)
        val absoluteErrors = errors.map(math.abs)
        
        val mse = squaredErrors.sum / targets.size.toDouble
        val mae = absoluteErrors.sum / targets.size.toDouble
        
        // R-squared calculation
        val meanTarget = targets.sum / targets.size.toDouble
        val totalSumSquares = targets.map(t => pow(t - meanTarget, 2)).sum
        val residualSumSquares = squaredErrors.sum
        val rSquared = if (totalSumSquares > 1e-10) 1 - (residualSumSquares / totalSumSquares) else 0.0
        
        ModelMetrics(
          mse = mse,
          rmse = sqrt(mse),
          mae = mae,
          rSquared = rSquared,
          trainingTime = lastTrainingTime,
          predictionTime = predictionTime,
          iterationsCompleted = iterationsCompleted,
          convergenceValue = trainingHistory.lastOption.getOrElse(0.0),
          trainingHistory = trainingHistory
        )
      }
    }
  }
  
  def save(filePath: String): Try[Unit] = {
    if (!isTrained) {
      return Failure(new RuntimeException("Cannot save untrained model"))
    }
    
    Try {
      val modelData = Map(
        "weights" -> weights,
        "bias" -> bias,
        "config" -> config,
        "trainingHistory" -> trainingHistory,
        "trainingTime" -> lastTrainingTime,
        "iterationsCompleted" -> iterationsCompleted
      )
      
      val json = modelData.toJson.prettyPrint
      Files.write(Paths.get(filePath), json.getBytes)
      logger.info(s"Model saved to $filePath")
    }
  }
  
  def load(filePath: String): Try[Unit] = {
    Try {
      val jsonString = new String(Files.readAllBytes(Paths.get(filePath)))
      val modelData = jsonString.parseJson.asJsObject.fields
      
      // Pattern matching for safe extraction
      weights = modelData("weights").convertTo[Vec]
      bias = modelData("bias").convertTo[Double]
      trainingHistory = modelData.get("trainingHistory").map(_.convertTo[List[Double]]).getOrElse(List.empty)
      iterationsCompleted = modelData.get("iterationsCompleted").map(_.convertTo[Int]).getOrElse(0)
      _isTrained = true
      
      logger.info(s"Model loaded from $filePath")
    }
  }
  
  private def performGradientDescent(features: Matrix, targets: Vec): Unit = {
    var prevCost = Double.MaxValue
    var patienceCounter = 0
    
    for (iteration <- 0 until config.maxIterations) {
      // Forward pass using functional operations
      val predictions = computePredictions(features)
      
      // Compute cost
      val cost = computeCost(predictions, targets)
      trainingHistory = trainingHistory :+ cost
      
      // Check convergence
      if (math.abs(prevCost - cost) < config.convergenceThreshold) {
        logger.info(s"Convergence achieved at iteration $iteration")
        return
      }
      
      // Early stopping check using pattern matching
      if (config.enableEarlyStopping) {
        if (cost > prevCost) {
          patienceCounter += 1
          if (patienceCounter >= config.earlyStoppingPatience) {
            logger.info(s"Early stopping at iteration $iteration")
            return
          }
        } else {
          patienceCounter = 0
        }
      }
      
      prevCost = cost
      
      // Backward pass
      updateParameters(features, predictions, targets)
      iterationsCompleted = iteration + 1
    }
  }
  
  private def computePredictions(features: Matrix): Vec = {
    features.map(sample => bias + MathUtils.dotProduct(sample, weights))
  }
  
  private def computeCost(predictions: Vec, targets: Vec): Double = {
    val errors = predictions.zip(targets).map { case (p, t) => p - t }
    var cost = errors.map(e => e * e).sum / (2.0 * targets.size)
    
    // Add regularization if enabled using functional approach
    if (config.enableRegularization) {
      val regularization = config.regularizationStrength * weights.map(w => w * w).sum
      cost += regularization
    }
    
    cost
  }
  
  private def updateParameters(features: Matrix, predictions: Vec, targets: Vec): Unit = {
    val samples = features.size.toDouble
    
    // Compute gradients using functional operations
    val errors = predictions.zip(targets).map { case (p, t) => p - t }
    val biasGradient = errors.sum
    
    val weightGradients = (0 until weights.size).map { j =>
      features.zip(errors).map { case (sample, error) => error * sample(j) }.sum
    }
    
    // Update parameters
    bias = bias - config.learningRate * biasGradient / samples
    
    weights = weights.zipWithIndex.map { case (weight, j) =>
      var gradient = weightGradients(j) / samples
      
      // Add regularization gradient if enabled
      if (config.enableRegularization) {
        gradient += config.regularizationStrength * weight
      }
      
      weight - config.learningRate * gradient
    }
  }
}

// MARK: - Production ML Pipeline

class EnterpriseMLPipeline(
  model: MLModel = new EnterpriseLinearRegression(),
  validator: EnterpriseDataValidator = new EnterpriseDataValidator(),
  logger: MLLogger = new ConsoleLogger
)(implicit ec: ExecutionContext) {
  
  private val featureEngineer = new AdvancedFeatureEngineer(logger)
  private var lastTransformation: Option[FeatureTransformResult] = None
  private var isStandardized = false
  private var isTraining = false
  
  /**
   * Train the complete ML pipeline using Future composition
   */
  def train(features: Matrix, targets: Vec, validationSplit: Double = 0.2): Future[Unit] = {
    val monitor = new PerformanceMonitor("Enterprise Pipeline Training", logger)
    
    if (isTraining) {
      return Future.failed(ModelTrainingError("Pipeline training already in progress"))
    }
    
    isTraining = true
    
    val trainingFuture = for {
      // Data validation
      _ <- Future.successful(logger.info("Starting data validation..."))
      validation <- validator.validate(features, Some(targets))
      _ <- if (!validation.isValid) {
        val errorMsg = s"Data validation failed: ${validation.errors.mkString("; ")}"
        Future.failed(DataValidationError(errorMsg, validation.errors))
      } else Future.successful(())
      
      // Feature standardization
      _ <- Future.successful(logger.info("Applying feature standardization..."))
      transformation <- featureEngineer.standardizeFeatures(features)
      _ <- Future.successful {
        lastTransformation = Some(transformation)
        isStandardized = true
      }
      
      // Train-validation split
      (trainFeatures, valFeatures, trainTargets, valTargets) = MathUtils.trainTestSplit(
        transformation.transformedFeatures, targets, validationSplit
      )
      
      // Model training
      _ <- Future.successful(logger.info("Starting model training..."))
      _ <- Future.fromTry(model.train(trainFeatures, trainTargets))
      
      // Validation evaluation
      _ <- if (validationSplit > 0) {
        logger.info("Evaluating on validation set...")
        model.evaluate(valFeatures, valTargets) match {
          case Success(metrics) =>
            logger.info(f"Validation RÂ²: ${metrics.rSquared}%.4f, RMSE: ${metrics.rmse}%.4f")
            Future.successful(())
          case Failure(ex) =>
            Future.failed(ex)
        }
      } else Future.successful(())
      
      _ <- Future.successful(logger.info("Pipeline training completed successfully"))
    } yield ()
    
    trainingFuture.andThen {
      case _ => 
        isTraining = false
        monitor.dispose()
    }
  }
  
  /**
   * Make predictions using the trained pipeline
   */
  def predict(features: Matrix): Try[Vec] = {
    if (!model.isTrained) {
      return Failure(ModelPredictionError("Pipeline must be trained before making predictions"))
    }
    
    Try {
      val processedFeatures = if (isStandardized && lastTransformation.isDefined) {
        applyStandardization(features, lastTransformation.get)
      } else features
      
      model.predict(processedFeatures) match {
        case Success(predictions) => predictions
        case Failure(ex) => throw ex
      }
    }.recoverWith {
      case ex =>
        logger.logException(ex, "Pipeline prediction failed")
        Failure(ex)
    }
  }
  
  /**
   * Evaluate the pipeline performance
   */
  def evaluate(features: Matrix, targets: Vec): Try[ModelMetrics] = {
    Try {
      val processedFeatures = if (isStandardized && lastTransformation.isDefined) {
        applyStandardization(features, lastTransformation.get)
      } else features
      
      model.evaluate(processedFeatures, targets) match {
        case Success(metrics) => metrics
        case Failure(ex) => throw ex
      }
    }.recoverWith {
      case ex =>
        logger.logException(ex, "Pipeline evaluation failed")
        Failure(ex)
    }
  }
  
  /**
   * Save the complete pipeline
   */
  def savePipeline(directoryPath: String): Try[Unit] = {
    Try {
      val dir = new File(directoryPath)
      if (!dir.exists()) {
        dir.mkdirs()
      }
      
      // Save model
      model.save(s"$directoryPath/model.json").get
      
      // Save feature transformation parameters
      lastTransformation.foreach { transformation =>
        val transformData = Map(
          "isStandardized" -> isStandardized,
          "featureMeans" -> transformation.featureMeans,
          "featureStds" -> transformation.featureStds,
          "transformationParameters" -> transformation.transformationParameters
        )
        
        val json = transformData.toJson.prettyPrint
        Files.write(Paths.get(s"$directoryPath/feature_transform.json"), json.getBytes)
      }
      
      logger.info(s"Pipeline saved to $directoryPath")
    }
  }
  
  private def applyStandardization(features: Matrix, transformation: FeatureTransformResult): Matrix = {
    (transformation.featureMeans, transformation.featureStds) match {
      case (Some(means), Some(stds)) =>
        features.map { sample =>
          sample.zipWithIndex.map { case (value, j) =>
            (value - means(j)) / stds(j)
          }
        }
      case _ => features
    }
  }
  
  def getPipelineStatus: Map[String, Any] = {
    Map(
      "isModelTrained" -> model.isTrained,
      "isStandardized" -> isStandardized,
      "isTraining" -> isTraining
    )
  }
}

// MARK: - Demonstration Object

object ScalaMLPatterns {
  
  implicit val ec: ExecutionContext = scala.concurrent.ExecutionContext.global
  
  /**
   * Comprehensive demonstration of Scala ML patterns using functional programming
   */
  def demonstrateScalaMLPatterns(): Unit = {
    val logger = new ConsoleLogger
    
    try {
      logger.info("ðŸš€ Scala ML Production Patterns Demonstration")
      logger.info("==============================================")
      
      // Generate synthetic dataset
      logger.info("ðŸ“Š Generating synthetic dataset...")
      val (features, targets) = MathUtils.generateRegressionDataset(1000, 5, noiseLevel = 0.1)
      
      // Create enterprise pipeline
      logger.info("ðŸ—ï¸ Creating enterprise ML pipeline...")
      val config = TrainingConfig(
        learningRate = 0.01,
        maxIterations = 1000,
        convergenceThreshold = 1e-6,
        validationSplit = 0.2,
        enableEarlyStopping = true,
        earlyStoppingPatience = 10
      )
      
      val pipeline = new EnterpriseMLPipeline(
        model = new EnterpriseLinearRegression(config, logger),
        validator = new EnterpriseDataValidator(logger = logger),
        logger = logger
      )
      
      // Train pipeline using Future
      logger.info("ðŸ”„ Training production ML pipeline...")
      val trainingFuture = pipeline.train(features, targets, validationSplit = 0.2)
      
      // Block for demonstration (in real application, use proper async handling)
      import scala.concurrent.Await
      import scala.concurrent.duration._
      
      Await.result(trainingFuture, 30.seconds)
      logger.info("âœ… Model training completed")
      
      // Make predictions
      logger.info("ðŸ”® Making predictions...")
      val (testFeatures, testTargets) = MathUtils.generateRegressionDataset(100, 5, noiseLevel = 0.1, seed = 123)
      
      pipeline.predict(testFeatures) match {
        case Success(predictions) =>
          val samplePredictions = predictions.take(5).map(p => f"$p%.4f").mkString(", ")
          logger.info(s"Sample predictions: $samplePredictions")
        case Failure(ex) =>
          logger.logException(ex, "Prediction failed")
          return
      }
      
      // Model evaluation
      logger.info("ðŸ“Š Evaluating model performance...")
      pipeline.evaluate(testFeatures, testTargets) match {
        case Success(metrics) =>
          logger.info(f"RÂ² Score: ${metrics.rSquared}%.4f")
          logger.info(f"RMSE: ${metrics.rmse}%.4f")
          logger.info(f"MAE: ${metrics.mae}%.4f")
          logger.info(f"Training Time: ${metrics.trainingTime}%.2f seconds")
          logger.info(f"Prediction Time: ${metrics.predictionTime * 1000}%.2fms")
        case Failure(ex) =>
          logger.logException(ex, "Evaluation failed")
          return
      }
      
      // Feature engineering demonstration
      logger.info("ðŸ”§ Feature Engineering demonstration...")
      val featureEngineer = new AdvancedFeatureEngineer(logger)
      val polynomialFuture = featureEngineer.createPolynomialFeatures(testFeatures, degree = 2)
      
      Await.result(polynomialFuture, 10.seconds) match {
        case polynomialResult =>
          logger.info(s"Original features: ${testFeatures.headOption.map(_.size).getOrElse(0)}, " +
                     s"Polynomial features: ${polynomialResult.transformedFeatures.headOption.map(_.size).getOrElse(0)}")
      }
      
      // Performance monitoring summary
      logger.info("âš¡ Performance characteristics:")
      logger.info("- Functional programming: âœ… Immutable data structures and pure functions")
      logger.info("- Type safety: âœ… Case classes, sealed traits, and pattern matching")
      logger.info("- Parallel processing: âœ… Parallel collections and concurrent futures")
      logger.info("- Error handling: âœ… Try, Either, and Option monads")
      logger.info("- JVM integration: âœ… Seamless Java interop and ecosystem access")
      
      logger.info("âœ… Scala ML demonstration completed successfully!")
      
    } catch {
      case ex: Throwable =>
        logger.logException(ex, "Fatal error during demonstration")
        throw ex
    }
  }
  
  def main(args: Array[String]): Unit = {
    demonstrateScalaMLPatterns()
  }
}

// MARK: - Main Entry Point

object Main extends App {
  ScalaMLPatterns.demonstrateScalaMLPatterns()
}